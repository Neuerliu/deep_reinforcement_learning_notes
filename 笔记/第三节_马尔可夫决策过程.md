# 马尔可夫决策过程

## 3.1 简介

马尔可夫决策过程(Markov decision process，MDP)是强化学习的重要概念。**强化学习中的环境一般就是一个马尔可夫决策过程**。

与多臂老虎机问题不同，马尔可夫决策过程包含**状态信息**以及状态之间的**转移机制**。如果要用强化学习去解决一个实际问题，第一步要做的事情就是把这个实际问题抽象为一个马尔可夫决策过程，也就是明确马尔可夫决策过程的各个组成要素。

## 3.2 马尔可夫过程

### 3.2.1 随机过程

随机过程(stochastic process)的研究对象是**随时间演变的随机现象**，例如天气随时间的变化、城市交通随时间的变化。在随机过程中，随机现象在某时刻的取值是一个向量随机变量，用 $S_t$ 表示，所有可能的状态组成状态集合 $S$ 。随机现象便是状态的变化过程。在某时刻 $t$ 的状态 $S_t$ 通常取决于时刻之前的状态。我们将已知历史信息 $(S_1,..., S_t)$ 下一个时刻状态 $S_{t+1}$ 为的概率表示成 $P(S_{t+1}|S_1, ..., S_t)$。

### 3.2.2 马尔可夫性质

一个随机过程被称为具有**马尔可夫性质**(Markov property)，当且仅当某时刻的状态**只取决于上一时刻的状态**，用公式表示为
$$
P(S_{t+1}|S_t) = P(S_{t+1}|S_1, ..., S_t)
$$
也就是说，当前状态是未来的**充分统计量**，即下一个状态只取决于当前状态，而不会受到过去状态的影响。需要明确的是，具有马尔可夫性并不代表这个随机过程就和历史完全没有关系。因为虽然 $t+1$ 时刻的状态只与 $t$ 时刻的状态有关，但是 $t$ 时刻的状态其实包含了 $t-1$ 时刻的状态的信息，**通过这种链式的关系，历史的信息被传递到了现在**。

### 3.2.3 马尔可夫过程

**马尔可夫过程**(Markov process)指具有马尔可夫性质的随机过程，也被称为**马尔可夫链**(Markov chain)。我们通常用**元组 $<\mathcal{S}, \mathcal{P}>$** 描述一个马尔可夫过程，其中 $\mathcal{S}$ 是有限数量的**状态集合**， $\mathcal{P}$ 是**状态转移矩阵**(state transition matrix)。

假设一共有 $n$ 个状态，此时 $S=\{s_1, ..., s_n\}$ 。状态转移矩阵 $\mathcal{P}$ 定义了所有状态对之间的转移概率，即
$$
P=\begin{bmatrix}
P\{s_1|s_1\} & \cdots & P\{s_n|s_1\} \\
\vdots & \ddots & \vdots \\
P\{s_1|s_n\} & \cdots & P\{s_n|s_n\}
\end{bmatrix}
$$
矩阵的 $P_{ij}$ 元素表示 $P(s_j|s_i)=P(S_{t+1}=s_j|S_t=s_i)$表示从状态 $s_i$ 转移到状态 $s_j$ 的概率。由于从某个状态出发，到达其他状态的概率和必须为1，因此
$$
\sum_{j=1}^{n}P_{ij}=1
$$
以下展示一个具有6个状态的马尔可夫过程，其中 $S_6$ 被称为终止状态，因为它不会转移到其他状态。

![img](https://hrl.boyuai.com/static/markov-process.487c21a3.png)

给定一个马尔可夫过程，我们就可以从某个状态出发，根据它的状态转移矩阵生成一个状态**序列**(episode)，这个步骤也被叫做**采样**(sampling)。

## 3.3 马尔可夫奖励过程

在马尔可夫过程的基础上加入奖励函数 $r$ 和折扣因子 $\gamma$ ，就可以得到**马尔可夫奖励过程**(Markov reward process)。一个马尔可夫奖励过程由 $<\mathcal{S}, \mathcal{P}, r, \gamma>$ 构成，各个组成元素的含义如下所示：

- $\mathcal{S}$ 是**有限状态的集合**
- $\mathcal{P}$ 是**状态转移矩阵**
-  $r$ 是**奖励函数**，某个状态 $s$ 的奖励 $r(s)$ 指转移到该状态时可以获得奖励的期望

- $\gamma$ 是**折扣因子**(discount factor)，$\gamma$ 的取值范围为 $[0,1)$ 。引入折扣因子的理由是**远期利益具有一定不确定性**，所以我们需要对远期利益打一些折扣——接近1的 $\gamma$ 更关注长期的累计奖励，接近0的 $\gamma$ 更考虑短期奖励

### 3.3.1 回报

在一个马尔可夫奖励过程中，从第 $t$ 时刻状态 $S_t$ 开始，直到终止状态时，所有奖励的衰减之和称为**回报 $G_t$ **(Return)，公式如下：
$$
G_t = R_t + \gamma R_{t+1} + \gamma^2 R_{t+2} + ... = \sum_{k=0}^{\infty}{\gamma^k R_{t+k}}
$$
其中，$R_t$ 表示在时刻 $t$ 获得的奖励。下图继续使用之前马尔可夫过程的例子，并在其基础上添加**奖励函数**，构建成一个马尔可夫奖励过程。

![img](https://hrl.boyuai.com/static/mrp.c1e62649.png)

下面用代码表示图中马尔可夫奖励过程：

```python
import numpy as np

# 定义随机数种子
np.random.seed(0)
# 定义状态转移概率矩阵P
P = [
    [0.9, 0.1, 0.0, 0.0, 0.0, 0.0],
    [0.5, 0.0, 0.5, 0.0, 0.0, 0.0],
    [0.0, 0.0, 0.0, 0.6, 0.0, 0.4],
    [0.0, 0.0, 0.0, 0.0, 0.3, 0.7],
    [0.0, 0.2, 0.3, 0.5, 0.0, 0.0],
    [0.0, 0.0, 0.0, 0.0, 0.0, 1.0],
]
P = np.array(P)

rewards = [-1, -2, -2, 10, 1, 0] # 定义各状态的奖励函数
gamma = 0.5 # 折扣因子

# 计算给定序列上，从某个索引开始到序列结束的回报
def compute_return(start_index, chain, gamma):
    '''
    计算回报，采用回溯计算的方式
    '''
    G = 0 # 总回报
    for i in reversed(range(start_index, len(chain))):
        G = G * gamma + rewards[chain[i]-1]

    return G

# 一个状态序列,s1-s2-s3-s6
chain = [1, 2, 3, 6]
start_index = 0
G = compute_return(start_index, chain, gamma)
print("根据本序列计算得到回报为：%s。" % G)
```

### 3.3.2 价值函数

在马尔可夫奖励过程中，一个状态的**期望回报**(即从这个状态出发的未来累积奖励的期望)被称为这个状态的**价值**(value)。所有状态的价值就组成了**价值函数**(value function)，价值函数的输入为某个状态，输出为这个状态的价值。我们将价值函数写成 $V(s) = \mathbb{E}[G_t|S_t=s]$ ，展开为
$$
V(s) = \mathbb{E}[G_t|S_t=s] = \mathbb{E}[R_t + \gamma R_{t+1} + ... | S_t=s] = \mathbb{E}[R_t + \gamma G_{t+1}| S_t=s]
$$
上式可以进一步简化：

- 即时奖励的期望正是奖励函数的输出，即 $\mathbb{E} [R_t|S_t=s] = r(s)$
- 等式中剩余部分 $\mathbb{E}[\gamma G_{t+1}| S_t=s]$ 可以根据从状态 $s$ 出发的转移概率得到，即 $\mathbb{E}[\gamma G_{t+1}| S_t=s]=\gamma \mathbb{E}[G_{t+1}| S_t=s]=\gamma \sum_{s^{'}\in S}{p(s^{'}|s)V(s^{'})}$ 

$$
V(s) = r(s) + \gamma \sum_{s^{'}\in S}{p(s^{'}|s)V(s^{'})}
$$

上式就是马尔可夫奖励过程中非常有名的**贝尔曼方程**(Bellman equation)。若一个马尔可夫奖励过程一共有 $n$ 个状态，即 $S=\{s_1, s_2, ..., s_n\}$ ，我们将所有状态的价值表示成一个列向量 $\mathcal{V} = [V(s_1), V(s_2), ..., V(s_n)]^T$，同理，将奖励函数写成一个列向量 $\mathcal{R} = [r(s_1), r(s_2), ..., r(s_n)]^T$ 。此时可以将贝尔曼方程写成矩阵的形式：
$$
\mathcal{V} = \mathcal{R} + \gamma \mathcal{P} \mathcal{V}
$$
可以直接对上式进行求解，得到：
$$
(I - \gamma \mathcal{P})\mathcal{V} = \mathcal{R} \\
\mathcal{V} = (I-\gamma P)^{-1}\mathcal{R}
$$
上述方法求解的计算复杂度高，只能用于小规模的马尔可夫奖励过程。对于大规模情况，可以使用动态规划、蒙特卡洛、时序差分算法。

以下编写求解马尔可夫价值函数的代码：

```python
# 求解马尔可夫价值函数
def compute()
```

