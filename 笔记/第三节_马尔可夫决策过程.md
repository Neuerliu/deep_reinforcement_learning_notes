# 马尔可夫决策过程

## 3.1 简介

马尔可夫决策过程(Markov decision process，MDP)是强化学习的重要概念。**强化学习中的环境一般就是一个马尔可夫决策过程**。

与多臂老虎机问题不同，马尔可夫决策过程包含**状态信息**以及状态之间的**转移机制**。如果要用强化学习去解决一个实际问题，第一步要做的事情就是把这个实际问题抽象为一个马尔可夫决策过程，也就是明确马尔可夫决策过程的各个组成要素。

## 3.2 马尔可夫过程

### 3.2.1 随机过程

随机过程(stochastic process)的研究对象是**随时间演变的随机现象**，例如天气随时间的变化、城市交通随时间的变化。在随机过程中，随机现象在某时刻的取值是一个向量随机变量，用 $S_t$ 表示，所有可能的状态组成状态集合 $S$ 。随机现象便是状态的变化过程。在某时刻 $t$ 的状态 $S_t$ 通常取决于时刻之前的状态。我们将已知历史信息 $(S_1,..., S_t)$ 下一个时刻状态 $S_{t+1}$ 为的概率表示成 $P(S_{t+1}|S_1, ..., S_t)$。

### 3.2.2 马尔可夫性质

一个随机过程被称为具有**马尔可夫性质**(Markov property)，当且仅当某时刻的状态**只取决于上一时刻的状态**，用公式表示为
$$
P(S_{t+1}|S_t) = P(S_{t+1}|S_1, ..., S_t)
$$
也就是说，当前状态是未来的**充分统计量**，即下一个状态只取决于当前状态，而不会受到过去状态的影响。需要明确的是，具有马尔可夫性并不代表这个随机过程就和历史完全没有关系。因为虽然 $t+1$ 时刻的状态只与 $t$ 时刻的状态有关，但是 $t$ 时刻的状态其实包含了 $t-1$ 时刻的状态的信息，**通过这种链式的关系，历史的信息被传递到了现在**。

### 3.2.3 马尔可夫过程

**马尔可夫过程**(Markov process)指具有马尔可夫性质的随机过程，也被称为**马尔可夫链**(Markov chain)。我们通常用**元组 $<\mathcal{S}, \mathcal{P}>$** 描述一个马尔可夫过程，其中 $\mathcal{S}$ 是有限数量的**状态集合**， $\mathcal{P}$ 是**状态转移矩阵**(state transition matrix)。

假设一共有 $n$ 个状态，此时 $S=\{s_1, ..., s_n\}$ 。状态转移矩阵 $\mathcal{P}$ 定义了所有状态对之间的转移概率，即
$$
P=\begin{bmatrix}
P\{s_1|s_1\} & \cdots & P\{s_n|s_1\} \\
\vdots & \ddots & \vdots \\
P\{s_1|s_n\} & \cdots & P\{s_n|s_n\}
\end{bmatrix}
$$
矩阵的 $P_{ij}$ 元素表示 $P(s_j|s_i)=P(S_{t+1}=s_j|S_t=s_i)$表示从状态 $s_i$ 转移到状态 $s_j$ 的概率。由于从某个状态出发，到达其他状态的概率和必须为1，因此
$$
\sum_{i=1}^{n}P_{ij}=1
$$
以下展示一个具有6个状态的马尔可夫过程，其中 $S_6$ 被称为终止状态，因为它不会转移到其他状态。

![img](https://hrl.boyuai.com/static/markov-process.487c21a3.png)

给定一个马尔可夫过程，我们就可以从某个状态出发，根据它的状态转移矩阵生成一个状态**序列**(episode)，这个步骤也被叫做**采样**(sampling)。

## 3.3 马尔可夫奖励过程

在马尔可夫过程的基础上加入奖励函数 $r$ 和折扣因子 $\gamma$ ，就可以得到**马尔可夫奖励过程**(Markov reward process)。一个马尔可夫奖励过程由 $<\mathcal{S}, \mathcal{P}, r, \gamma>$ 构成，各个组成元素的含义如下所示：

- $\mathcal{S}$ 是**有限状态的集合**
- $\mathcal{P}$ 是**状态转移矩阵**
-  $r$ 是**奖励函数**，某个状态 $s$ 的奖励 $r(s)$ 指转移到该状态时可以获得奖励的期望

- $\gamma$ 是**折扣因子**(discount factor)，$\gamma$ 的取值范围为 $[0,1)$ 。引入折扣因子的理由是**远期利益具有一定不确定性**，所以我们需要对远期利益打一些折扣——接近1的 $\gamma$ 更关注长期的累计奖励，接近0的 $\gamma$ 更考虑短期奖励

### 3.3.1 回报