# 马尔可夫决策过程

## 3.1 简介

马尔可夫决策过程(Markov decision process，MDP)是强化学习的重要概念。**强化学习中的环境一般就是一个马尔可夫决策过程**。

与多臂老虎机问题不同，马尔可夫决策过程包含**状态信息**以及状态之间的**转移机制**。如果要用强化学习去解决一个实际问题，第一步要做的事情就是把这个实际问题抽象为一个马尔可夫决策过程，也就是明确马尔可夫决策过程的各个组成要素。

## 3.2 马尔可夫过程

### 3.2.1 随机过程

随机过程(stochastic process)的研究对象是**随时间演变的随机现象**，例如天气随时间的变化、城市交通随时间的变化。在随机过程中，随机现象在某时刻的取值是一个向量随机变量，用 $S_t$ 表示，所有可能的状态组成状态集合 $S$ 。随机现象便是状态的变化过程。在某时刻 $t$ 的状态 $S_t$ 通常取决于时刻之前的状态。我们将已知历史信息 $(S_1,..., S_t)$ 下一个时刻状态 $S_{t+1}$ 为的概率表示成 $P(S_{t+1}|S_1, ..., S_t)$。

### 3.2.2 马尔可夫性质

一个随机过程被称为具有**马尔可夫性质**(Markov property)，当且仅当某时刻的状态**只取决于上一时刻的状态**，用公式表示为
$$
P(S_{t+1}|S_t) = P(S_{t+1}|S_1, ..., S_t)
$$
也就是说，当前状态是未来的**充分统计量**，即下一个状态只取决于当前状态，而不会受到过去状态的影响。需要明确的是，具有马尔可夫性并不代表这个随机过程就和历史完全没有关系。因为虽然 $t+1$ 时刻的状态只与 $t$ 时刻的状态有关，但是 $t$ 时刻的状态其实包含了 $t-1$ 时刻的状态的信息，**通过这种链式的关系，历史的信息被传递到了现在**。

### 3.2.3 马尔可夫过程

**马尔可夫过程**(Markov process)指具有马尔可夫性质的随机过程，也被称为**马尔可夫链**(Markov chain)。我们通常用**元组 $<\mathcal{S}, \mathcal{P}>$** 描述一个马尔可夫过程，其中 $\mathcal{S}$ 是有限数量的**状态集合**， $\mathcal{P}$ 是**状态转移矩阵**(state transition matrix)。

假设一共有 $n$ 个状态，此时 $S=\{s_1, ..., s_n\}$ 。状态转移矩阵 $\mathcal{P}$ 定义了所有状态对之间的转移概率，即
$$
P=\begin{bmatrix}
P\{s_1|s_1\} & \cdots & P\{s_n|s_1\} \\
\vdots & \ddots & \vdots \\
P\{s_1|s_n\} & \cdots & P\{s_n|s_n\}
\end{bmatrix}
$$
矩阵的 $P_{ij}$ 元素表示 $P(s_j|s_i)=P(S_{t+1}=s_j|S_t=s_i)$表示从状态 $s_i$ 转移到状态 $s_j$ 的概率。由于从某个状态出发，到达其他状态的概率和必须为1，因此
$$
\sum_{j=1}^{n}P_{ij}=1
$$
以下展示一个具有6个状态的马尔可夫过程，其中 $S_6$ 被称为终止状态，因为它不会转移到其他状态。

![img](https://hrl.boyuai.com/static/markov-process.487c21a3.png)

给定一个马尔可夫过程，我们就可以从某个状态出发，根据它的状态转移矩阵生成一个状态**序列**(episode)，这个步骤也被叫做**采样**(sampling)。

## 3.3 马尔可夫奖励过程

在马尔可夫过程的基础上加入奖励函数 $r$ 和折扣因子 $\gamma$ ，就可以得到**马尔可夫奖励过程**(Markov reward process)。一个马尔可夫奖励过程由 $<\mathcal{S}, \mathcal{P}, r, \gamma>$ 构成，各个组成元素的含义如下所示：

- $\mathcal{S}$ 是**有限状态的集合**
- $\mathcal{P}$ 是**状态转移矩阵**
-  $r$ 是**奖励函数**，某个状态 $s$ 的奖励 $r(s)$ 指转移到该状态时可以获得奖励的期望

- $\gamma$ 是**折扣因子**(discount factor)，$\gamma$ 的取值范围为 $[0,1)$ 。引入折扣因子的理由是**远期利益具有一定不确定性**，所以我们需要对远期利益打一些折扣——接近1的 $\gamma$ 更关注长期的累计奖励，接近0的 $\gamma$ 更考虑短期奖励

### 3.3.1 回报

在一个马尔可夫奖励过程中，从第 $t$ 时刻状态 $S_t$ 开始，直到终止状态时，所有奖励的衰减之和称为**回报 $G_t$ **(Return)，公式如下：
$$
G_t = R_t + \gamma R_{t+1} + \gamma^2 R_{t+2} + ... = \sum_{k=0}^{\infty}{\gamma^k R_{t+k}}
$$
其中，$R_t$ 表示在时刻 $t$ 获得的奖励。下图继续使用之前马尔可夫过程的例子，并在其基础上添加**奖励函数**，构建成一个马尔可夫奖励过程。

![img](https://hrl.boyuai.com/static/mrp.c1e62649.png)

下面用代码表示图中马尔可夫奖励过程：

```python
import numpy as np

# 定义随机数种子
np.random.seed(0)
# 定义状态转移概率矩阵P
P = [
    [0.9, 0.1, 0.0, 0.0, 0.0, 0.0],
    [0.5, 0.0, 0.5, 0.0, 0.0, 0.0],
    [0.0, 0.0, 0.0, 0.6, 0.0, 0.4],
    [0.0, 0.0, 0.0, 0.0, 0.3, 0.7],
    [0.0, 0.2, 0.3, 0.5, 0.0, 0.0],
    [0.0, 0.0, 0.0, 0.0, 0.0, 1.0],
]
P = np.array(P)

rewards = [-1, -2, -2, 10, 1, 0] # 定义各状态的奖励函数
gamma = 0.5 # 折扣因子

# 计算给定序列上，从某个索引开始到序列结束的回报
def compute_return(start_index, chain, gamma):
    '''
    计算回报，采用回溯计算的方式
    '''
    G = 0 # 总回报
    for i in reversed(range(start_index, len(chain))):
        G = G * gamma + rewards[chain[i]-1]

    return G

# 一个状态序列,s1-s2-s3-s6
chain = [1, 2, 3, 6]
start_index = 0
G = compute_return(start_index, chain, gamma)
print("根据本序列计算得到回报为：%s。" % G)
```

### 3.3.2 价值函数

在马尔可夫奖励过程中，一个状态的**期望回报**(即从这个状态出发的未来累积奖励的期望)被称为这个状态的**价值**(value)。所有状态的价值就组成了**价值函数**(value function)，价值函数的输入为某个状态，输出为这个状态的价值。我们将价值函数写成 $V(s) = \mathbb{E}[G_t|S_t=s]$ ，展开为
$$
V(s) = \mathbb{E}[G_t|S_t=s] = \mathbb{E}[R_t + \gamma R_{t+1} + ... | S_t=s] = \mathbb{E}[R_t + \gamma G_{t+1}| S_t=s]
$$
上式可以进一步简化：

- 即时奖励的期望正是奖励函数的输出，即 $\mathbb{E} [R_t|S_t=s] = r(s)$
- 等式中剩余部分 $\mathbb{E}[\gamma G_{t+1}| S_t=s]$ 可以根据从状态 $s$ 出发的转移概率得到，即 $\mathbb{E}[\gamma G_{t+1}| S_t=s]=\gamma \mathbb{E}[G_{t+1}| S_t=s]=\gamma \sum_{s^{'}\in S}{p(s^{'}|s)V(s^{'})}$ 

$$
V(s) = r(s) + \gamma \sum_{s^{'}\in S}{p(s^{'}|s)V(s^{'})}
$$

上式就是马尔可夫奖励过程中非常有名的**贝尔曼方程**(Bellman equation)。若一个马尔可夫奖励过程一共有 $n$ 个状态，即 $S=\{s_1, s_2, ..., s_n\}$ ，我们将所有状态的价值表示成一个列向量 $\mathcal{V} = [V(s_1), V(s_2), ..., V(s_n)]^T$，同理，将奖励函数写成一个列向量 $\mathcal{R} = [r(s_1), r(s_2), ..., r(s_n)]^T$ 。此时可以将贝尔曼方程写成矩阵的形式：
$$
\mathcal{V} = \mathcal{R} + \gamma \mathcal{P} \mathcal{V}
$$
可以直接对上式进行求解，得到：
$$
(I - \gamma \mathcal{P})\mathcal{V} = \mathcal{R} \\
\mathcal{V} = (I-\gamma P)^{-1}\mathcal{R}
$$
上述方法求解的计算复杂度高，只能用于小规模的马尔可夫奖励过程。对于大规模情况，可以使用动态规划、蒙特卡洛、时序差分算法。

以下编写求解马尔可夫价值函数的代码：

```python
import numpy as np

# 定义随机数种子
np.random.seed(0)
# 定义状态转移概率矩阵P
P = [
    [0.9, 0.1, 0.0, 0.0, 0.0, 0.0],
    [0.5, 0.0, 0.5, 0.0, 0.0, 0.0],
    [0.0, 0.0, 0.0, 0.6, 0.0, 0.4],
    [0.0, 0.0, 0.0, 0.0, 0.3, 0.7],
    [0.0, 0.2, 0.3, 0.5, 0.0, 0.0],
    [0.0, 0.0, 0.0, 0.0, 0.0, 1.0],
]
P = np.array(P)

rewards = [-1, -2, -2, 10, 1, 0] # 定义各状态的奖励函数
gamma = 0.5 # 折扣因子

# 求解马尔可夫价值函数
def compute(P, rewards, gamma, states_num):
    '''
    计算贝尔曼方程的解析解
    '''
    rewards = np.array(rewards).reshape((-1,1))
    value = np.dot(np.linalg.inv(np.eye(states_num,states_num) - gamma * P), rewards)

    return value

V = compute(P, rewards, gamma, 6)
print("MRP中每个状态价值分别为\n", V)
```

## 3.4 马尔可夫决策过程

马尔可夫过程和马尔可夫奖励过程都是自发改变的随机过程；而如果有一个外界的“刺激”来共同改变这个随机过程，就有了**马尔可夫决策过程**(Markov decision process，MDP)。我们将这个来自外界的刺激称为**智能体**(agent)的动作，在马尔可夫奖励过程(MRP)的基础上加入动作，就得到了马尔可夫决策过程(MDP)。

**马尔可夫决策过程**由元组 $<\mathcal{S}, \mathcal{A}, \mathcal{P}, r, \gamma>$ 构成，其中：

- $\mathcal{S}$ 是状态的集合
- $\mathcal{A}$ 是动作的集合
- $P$ 是状态转移函数，其中 $P(s^{'}|s, a)$ 表示从状态 $s$ 采用动作 $a$ 到达状态 $s^{'}$ 的概率 
- $r$ 是奖励函数，这里 $r=r(s,a)$ 说明奖励同时取决于状态 $s$ 和动作 $a$
- $\gamma$ 是奖励的折扣因子

从上述概念可以看出MDP与MRP非常相像，主要区别为MDP中的**状态转移函数**和**奖励函数**都比MRP多了动作 $a$ 作为自变量。同时此时用状态转移函数 $P$ 代替了原本的状态转移矩阵，这样处理有两个优点：

- 状态转移函数和状态 $s$ 、动作 $a$ 均有关系，实际上变成了一个**三维数组**
- 状态转移函数更具有**一般性**，例如如果状态无限多(连续状态的MDP环境)，则状态转移矩阵难以表示

在马尔可夫决策过程(MDP)中，通常存在一个智能体来执行动作。马尔可夫决策过程是一个与时间相关的、不断进行的过程，在智能体和环境之间存在一个**不断交互**的过程。**交互过程**大致如下：

- 智能体根据当前状态 $S_t$ 选择动作 $A_t$
- 对于状态 $S_t$ 和动作 $A_t$，MDP根据奖励函数和状态转移函数得到 $S_{t+1}$ 和 $R_t$ 并反馈给智能体

![img](https://hrl.boyuai.com/static/rl-process.723b4a67.png)

### 3.4.1 策略

智能体的**策略**(Policy)通常用字母 $\pi$ 表示。策略 $\pi(a|s)=P(A_t=a|S_t=s)$ 是一个函数，表示在输入状态 $s$ 情况下采取动作 $a$ 的概率。

- 当一个策略是**确定性策略**时，它在每个状态时只输出一个**确定性的动作**，即只有该动作的概率为1，其他动作的概率为0
- 当一个策略是**随机性策略**时，它在每个状态时输出的是关于动作的概率分布，然后根据该分布进行**采样得到一个动作**

在MDP中，由于**马尔可夫性质**的存在，**策略只需要与当前状态有关，不需要考虑历史状态**。在 MDP 中也同样可以定义类似的价值函数，但此时的价值函数与策略有关，这意味着对于两个**不同的策略**来说，它们在同一个状态下的**价值也很可能是不同的**，可以这样理解：因为不同的策略会采取不同的动作，从而之后会遇到不同的状态，以及获得不同的奖励，所以它们的累积奖励的期望也就不同，即状态价值不同。

### 3.4.2 状态价值函数

我们用 $V^{\pi}(s)$ 表示在MDP中基于策略的**状态价值函数**(state-value function)，定义为从状态 $s$ 出发遵循策略 $\pi$ 能获得的**期望回报**，即：
$$
V^{\pi}(s) = \mathbb{E}_{\pi}{[G_t|S_t=s]}
$$

### 3.4.3 动作价值函数

不同于MRP，在MDP中，由于动作的存在，我们额外定义一个**动作价值函数**(action-value function)，用 $Q^{\pi}(s,a)$ 表示在MDP遵循策略 $\pi$ 时，对当前状态 $s$ 执行动作 $a$ 时的期望回报：
$$
Q^{\pi}(s,a) = \mathbb{E}_{\pi}[G_t|S_t=s, A_t=a]
$$
状态价值函数 $V^{\pi}(s)$ 和动作价值函数 $Q^{\pi}(s,a)$ 之间的关系：在使用策略 $\pi$ 时，状态 $s$ 的价值(基于状态价值函数定义的价值)等于在该状态下基于策略 $\pi$ 采取所有动作的概率与相应的动作价值相乘再求和的结果，即：
$$
V^{\pi}(s) = \sum_{a \in A}{\pi(a|s)Q^{\pi}(s,a)}
$$
比较下此时状态价值函数 $V^{\pi}(s)$ 和动作价值函数 $Q^{\pi}(s)$ 满足的贝尔曼方程：

- 状态价值函数 $V^{\pi}(s)$ 的贝尔曼方程

$$
V^{\pi}(s) = \sum_{a \in \mathcal{A}}{\pi(a|s)r(s,a)} + \gamma \sum_{a \in \mathcal{A}}{\pi(s|a)}\sum_{s^{'} \in \mathcal{S}}{P(s^{'}|s,a)V^{\pi}(s^{'})}
$$

- 动作价值函数 $Q^{\pi}(s)$ 的贝尔曼方程

$$
Q^{\pi}(s,a) = r(s,a) + \gamma \sum_{s^{'} \in \mathcal{S}}{P(s^{'}|s,a)V^{\pi}(s^{'})}
$$

### 3.4.4 贝尔曼期望方程

贝尔曼期望方程区别于贝尔曼方程，可以从上面的式子推导得到：
$$
V^{\pi}(s) = \sum_{a \in \mathcal{A}}{\pi(a|s)r(s,a)} + \gamma \sum_{a \in \mathcal{A}}{\pi(s|a)}\sum_{s^{'} \in \mathcal{S}}{P(s^{'}|s,a)V^{\pi}(s^{'})}
$$

$$
Q^{\pi}(s,a) = r(s,a) + \gamma \sum_{s^{'} \in \mathcal{S}}{P(s^{'}|s,a) \sum_{a^{'} \in \mathcal{A}}{\pi(a^{'}|s^{'}) Q^{\pi}(s^{'}, a^{'})}}
$$

以下面图中的案例为例，展示马尔可夫决策过程：

![img](https://hrl.boyuai.com/static/mdp.aaacb46a.png)

图中共有五种状态，每个状态可以采取实线对应的动作，采取动作后会以虚线旁的概率(如果没有，则为1)转移至下一个状态，并得到对应奖励。

针对MDP，我们希望将其转化为更为熟悉的MRP，以计算一个策略 $\pi$ 的状态价值函数 $V^{\pi}(s)$ 。采用的方式是将策略的动作进行边缘化，就可以得到没有动作的MRP，具体来说，我们希望用 $r^{'}(s) \rightarrow r(s,a)$ ，用 $P^{'}(s^{'}|s)\rightarrow P(s^{'}|s,a)$ 以转化为不考虑动作的 $MRP:<\mathcal{S}, \mathcal{P^{'}}, r^{'}, \gamma>$，其中
$$
r^{'}(s) = \sum_{a \in \mathcal{A}} \pi(a|s) r(s,a)
$$

$$
P^{'}(s^{'}|s) = \sum_{a \in \mathcal{A}} \pi(a|s) P(s^{'}|s,a)
$$

首先定义状态、动作等基本信息：

```python
import numpy as np

# 马尔可夫决策过程示例
# 定义状态和动作集合
S = ['s1', 's2', 's3', 's4', 's5']
A = ['保持s1', '前往s1', '前往s2', '前往s3', '前往s4', '前往s5', '概率前往']

# 状态转移函数
P = {
    "s1-保持s1-s1": 1.0,
    "s1-前往s2-s2": 1.0,
    "s2-前往s1-s1": 1.0,
    "s2-前往s3-s3": 1.0,
    "s3-前往s4-s4": 1.0,
    "s3-前往s5-s5": 1.0,
    "s4-前往s5-s5": 1.0,
    "s4-概率前往-s2": 0.2,
    "s4-概率前往-s3": 0.4,
    "s4-概率前往-s4": 0.4,
}

# 奖励函数
R = {
    "s1-保持s1": -1,
    "s1-前往s2": 0,
    "s2-前往s1": -1,
    "s2-前往s3": -2,
    "s3-前往s4": -2,
    "s3-前往s5": 0,
    "s4-前往s5": 10,
    "s4-概率前往": 1,
}

gamma = 0.5  # 折扣因子
MDP = (S, A, P, R, gamma) # 马尔可夫决策过程的集合
```

我们考虑采用两种策略，分别计算状态价值函数：

- 随机策略`pi_1`

```python
# 策略1,随机策略
Pi_1 = {
    "s1-保持s1": 0.5,
    "s1-前往s2": 0.5,
    "s2-前往s1": 0.5,
    "s2-前往s3": 0.5,
    "s3-前往s4": 0.5,
    "s3-前往s5": 0.5,
    "s4-前往s5": 0.5,
    "s4-概率前往": 0.5,
}
```

- 非随机策略`pi_2`

```python
# 策略2
Pi_2 = {
    "s1-保持s1": 0.6,
    "s1-前往s2": 0.4,
    "s2-前往s1": 0.3,
    "s2-前往s3": 0.7,
    "s3-前往s4": 0.5,
    "s3-前往s5": 0.5,
    "s4-前往s5": 0.1,
    "s4-概率前往": 0.9,
}
```

解决问题的基本思路是转化为存在解析解的`MRP`问题进行求解，代码如下：

```python
# 结合策略和状态转移函数，将MDP下的状态转移函数，转化为MRP下状态转移函数
def convert_P_mdp2mrp(P_mdp, Pi, S, A):
    '''
    将给定策略下的MDP的状态转移函数转化为MRP的状态转移函数

    输入:
    P_mdp:MDP下的状态转移函数
    Pi:给定策略
    S:状态列表
    A:动作列表
    '''
    state_len = len(S)
    P_mrp = np.zeros((state_len, state_len)) # MRP下的状态转移函数初始化
    for state_start in S:
        state_start_idx = S.index(state_start) # 起始状态索引
        for action in A:
            cur_state_action_pair = state_start + '-' + action# 当前的状态动作对

            # 找到策略中的状态动作对
            if cur_state_action_pair in Pi.keys():
                possibility = Pi[cur_state_action_pair] # 指定动作的概率
                state_end_dict = {k.split(cur_state_action_pair)[1][1:]:v for k, v in P_mdp.items() if cur_state_action_pair in k}

                for state_end, reward in state_end_dict.items():
                    state_end_idx = S.index(state_end) # 终止状态索引
                    P_mrp[state_start_idx, state_end_idx] += possibility * reward

    return P_mrp

# 将MDP下的奖励函数，转化为MRP下奖励函数
def convert_R_mdp2mrp(R_mdp, Pi, S, A):
    '''
    将给定策略下的MDP的状态转移函数转化为MRP的状态转移函数

    输入:
    P_mdp:MDP下的状态转移函数
    Pi:给定策略
    S:状态列表
    A:动作列表
    '''
    state_len = len(S)
    R_mrp = np.zeros(state_len) # MRP下的状态转移函数初始化
    for state_start in S:
        state_start_idx = S.index(state_start) # 起始状态索引
        for action in A:
            cur_state_action_pair = state_start + '-' + action# 当前的状态动作对

            # 找到策略中的状态动作对
            if cur_state_action_pair in Pi.keys():
                cur_state_action_pair_idx = [pi_key for pi_key in Pi.keys() if cur_state_action_pair in pi_key][0]
                possibility = Pi[cur_state_action_pair] # 指定动作的概率
                R_mrp[state_start_idx] += possibility * R_mdp[cur_state_action_pair_idx]

    return R_mrp

# 求解策略1下的状态转移函数，以此求解状态价值函数
P_from_mdp_to_mrp = convert_P_mdp2mrp(P, Pi_1, S, A)
R_from_mdp_to_mrp = convert_R_mdp2mrp(R, Pi_1, S, A)
V = compute(P_from_mdp_to_mrp, R_from_mdp_to_mrp, gamma, 5)
print("策略1 MDP中每个状态价值分别为\n", V)

# 求解策略2下的状态转移函数，以此求解状态价值函数
P_from_mdp_to_mrp = convert_P_mdp2mrp(P, Pi_2, S, A)
R_from_mdp_to_mrp = convert_R_mdp2mrp(R, Pi_2, S, A)
V = compute(P_from_mdp_to_mrp, R_from_mdp_to_mrp, gamma, 5)
print("策略2 MDP中每个状态价值分别为\n", V)
```

## 3.5 蒙特卡洛方法

**蒙特卡洛方法**(Monte-Carlo methods)也被称为统计模拟方法，是一种基于概率统计的数值计算方法。运用蒙特卡洛方法时，我们通常使用重复随机抽样，然后运用概率统计方法来从抽样结果中归纳出我们想求的目标的数值估计。

我们现在介绍如何用**蒙特卡洛方法来估计一个策略**在一个马尔可夫决策过程中的状**态价值函数**。直观的想法就是用策略 $\pi$ 在 MDP 上**采样很多条序列**，计算从这个状态出发的回报再求其期望就可以了，依据大数定理，公式如下：
$$
V^{\pi}(s) = \mathbb{E}_{\pi}{[G_t|S_t=s]} \approx \frac{1}{N} \sum_{i=1}^{N}{G_t^{(i)}}
$$
需要注意的问题是：在采样序列中，一个状态 $s$ 可能出现0次、1次或者多次，而计算一个状态的价值函数时，我们应该如何处理？有两种思路：

- 在该状态每一次出现时计算它的回报，最后用平均值作为该状态在此次采样的回报(**以下采用这种方式**)
- 一条序列只计算一次回报，也就是这条序列第一次出现该状态时计算后面的累积奖励，而后面再次出现该状态时，该状态就被忽略了

假设我们现在用策略 $\pi$ 从状态 $s$ 开始采样序列，据此来计算状态价值。我们为每一个状态维护一个计数器和总回报，计算状态价值的具体过程如下所示：

1. 使用策略 $\pi$ 采样若干序列，其中第 $i$ 条序列如下所示：

$$
s_0^{(i)} \stackrel{a_0^{(i)}}{\longrightarrow} r_0^{(i)}, s_1^{(i)} \stackrel{a_1^{(i)}}{\longrightarrow} ... \stackrel{a_{T-1}^{(i)}}{\longrightarrow} r_{T-1}^{(i)}, s_{T}^{(i)}
$$

2. 对每一条序列中的每一时间步的状态进行以下操作：
   - 更新状态 $s$ 的计数器 $N(s) \leftarrow N(s) + 1$
   - 更新状态 $s$ 的总回报 $M(s) \leftarrow M(s) + G_t$

下面我们定义采样函数，该函数接受状态转移函数和策略函数作为参数，返回一个序列，序列中包括每一步采样的元组结果 $(s, a, r, s\_next)$ ，直至达到终止序列。 

```python
def sample(MDP, Pi, timestep_max, number):
    '''
    蒙特卡洛采样序列

    输入:
    MDP:马尔可夫决策过程的五元素(S, A, P, R, gamma)
    Pi:策略
    timestep_max:最大时间步长
    number:采样个数
    '''
    S, A, P, R, gamma = MDP # 解包马尔可夫决策过程
    episodes = [] # 采样列表

    # 采样
    for _ in range(number):
        # 采样一条从开始到结束的序列
        episode = []
        timestep = 0 # 时间步长初始化
        s = S[np.random.randint(len(S))] # 随机选择状态作为初始状态
        # 序列，定义终止状态为s5
        while s != "s5" and timestep <= timestep_max:
            timestep += 1
            # 采样动作
            rand, temp = np.random.rand(), 0 # 采样动作的随机概率，采样动作的实际概率初始化
            for a_opt in A:
                temp += Pi.get(join(s, a_opt), 0) # 从策略中采样一个状态动作对的概率，如果不存在该状态动作对，概率默认为0
                if temp > rand:
                    # 采用该状态动作对
                    a = a_opt
                    r = R.get(join(s, a), 0) # 奖励
                    break

            # 采样下一个状态
            rand, temp = np.random.rand(), 0 # 采样状态的随机概率，采样状态的实际概率初始化
            for s_opt in S:
                temp += P.get(join(join(s, a), s_opt), 0) # 采样状态-动作-状态的实际概率
                if temp > rand:
                    # 采用该状态-动作-状态
                    s_next = s_opt
                    break

            episode.append((s, a, r, s_next))
            s = s_next

        # 记录该采样序列
        episodes.append(episode)

    return episodes
```

利用采样结果，可以通过蒙特卡洛方法逼近每个状态的价值，代码如下：

```python
# 对所有采样序列计算所有状态的价值
def MC(episodes, V, N, gamma):
    ''''
    依据多条采样序列，计算每个状态的价值

    输入:
    episodes:多条采样序列
    V:状态价值列表
    N:状态被访问的次数
    gamma:折扣因子
    '''
    for episode in episodes:
        G = 0 # 当前序列的回报
        # 序列从后往前计算
        for i in range(len(episode)-1, -1, -1):
            (s, a, r, s_next) = episode[i] # 当前时间步解包
            G = r + gamma * G # 奖励折扣
            N[s] = N[s] + 1 # 状态s访问次数更新
            V[s] = V[s] + (G - V[s]) / N[s] # 状态价值更新

timestep_max = 20
# 采样1000次,可以自行修改
episodes = sample(MDP, Pi_1, timestep_max, 1000)
V = {"s1": 0, "s2": 0, "s3": 0, "s4": 0, "s5": 0}
N = {"s1": 0, "s2": 0, "s3": 0, "s4": 0, "s5": 0}
MC(episodes, V, N, gamma)
print("使用蒙特卡洛方法计算MDP的状态价值为\n", V)
```

## 3.6 占用度量

不同策略的价值函数是不一样的，即使处于相同状态，不同策略下智能体到达该状态和接下来状态的概率都不同，因此价值函数不同。

首先我们定义MDP的初始状态分布为 $\nu_0(s)$ ，在有些资料中，初始状态分布会被定义在MDP的组成元素中。我们用 $P_{t}^{\pi}(s)$ 表示采取策略 $\pi$ 使得智能体在时刻 $t$ 状态为 $s$ 的概率，所以我们有 $P_{0}^{\pi}(s) = \nu_0(s)$ ，然后就可以定义一个策略的**状态访问分布**(state visitation distribution)：
$$
\nu^{\pi}(s) = (1-\gamma) \sum_{t=0}^{\infty}{\gamma^t P_{t}^{\pi}(s)}
$$
其中，$1-\gamma$ 是用来使得概率加和为1的归一化因子。状态访问概率表示一个策略和MDP交互会访问到的状态的分布。需要注意的是，理论上在计算该分布时需要交互到无穷步之后，但实际上智能体和MDP的交互在一个序列中是有限的。不过我们仍然可以用以上公式来表达状态访问概率的思想，状态访问概率有如下性质：
$$
\nu^{\pi}(s^{'}) = (1-\gamma)\nu_0(s^{'}) + \gamma \int{P(s^{'} | s,a) \pi(a|s) \nu^{\pi}(s) dsda} 
$$
此外，我们还可以定义策略的**占用度量**(occupancy measure)：
$$
\rho^{\pi}(s, a) = (1-\gamma)\sum_{t=0}^{\infty}{\gamma^t P^{\pi}_t(s) \pi(a|s)}
$$
它表示动作状态对 $(s, a)$ 被访问到的概率。二者之间存在如下关系：
$$
\rho^{\pi}(s, a) = \nu^{\pi}(s)\pi(a|s)
$$
针对以上两个式子，我们可以得到两个定理：

- 智能体分别以策略 $\pi_1$ 和 $\pi_2$ ，与同一个MDP交互得到的占用度量 $\rho^{\pi_1}$ 和 $\rho^{\pi_2}$满足：

$$
\rho^{\pi_1} = \rho^{\pi_2} \iff \pi_1 = \pi_2
$$

- 给定一合法占用度量 $\rho$ ，可生成该占用度量的唯一策略是：

$$
\pi_{\rho} = \frac{\rho(s, a)}{\sum_{a^{'}}{\rho(s, a^{'})}}
$$

接下来我们编写代码来**近似估计占用度量**。采用近似估计的方法是指设置一个较大的采样轨迹长度的最大值，然后采样很多次，用**状态动作对出现的频率估计实际概率**。

```python
def occupancy(episodes, s, a, timestep_max, gamma):
    '''
    计算状态动作对(s, a)的占用度量

    输入:
    episodes:采样的序列构成的列表
    s:状态
    a:动作
    timestep_max:采样序列的最大时间步长
    gamma:折扣因子
    '''
    rho = 0 # 占用度量初始化为0
    total_times = np.zeros(timestep_max) # 记录每个时间步t在采样序列中出现的次数
    occur_times = np.zeros(timestep_max) # 记录(s_t, a_t) = (s, a)的次数
    # 遍历采样序列表
    for episode in episodes:
        # 遍历当前序列各阶段
        for i in range(len(episode)):
            (s_opt, a_opt, r, s_next) = episode[i] # 访问序列当前状态、动作、奖励
            total_times[i] += 1 # 时间步i在当前采样序列中出现一次
            if s == s_opt and a == a_opt:
                occur_times[i] += 1

    # 采用倒序方式更新rho，更加方便
    for i in reversed(range(timestep_max)):
        if total_times[i]:
            rho += gamma ** i * occur_times[i] / total_times[i]

    return (1-gamma) * rho

# 采样
timestep_max = 1000
episodes_1 = sample(MDP, Pi_1, timestep_max, 1000) # 采样1000个
episodes_2 = sample(MDP, Pi_2, timestep_max, 1000) # 采样1000个

# 计算占用度量
rho_1 = occupancy(episodes_1, 's4', '概率前往', timestep_max, gamma)
rho_2 = occupancy(episodes_2, 's4', '概率前往', timestep_max, gamma)
print(rho_1, rho_2)
```

通过上述结果可以发现：不同策略对于同一状态动作对的占用度量是不同的。

## 3.7 最优策略

强化学习的通常目标是找到一个策略，使得智能体从初始状态出发能**获得最多的期望回报**。我们首先定义策略之间的偏序关系：
$$
\forall s, V^{\pi}(s) \geq v^{\pi^{'}}(s),则记\pi > \pi^{'}
$$
因此，在有限状态和动作集合中，至少存在一个策略不差于其他所有策略，则我们记这个策略为**最优策略**，符号为 $\pi^{*}(s)$ 。最优策略都有相同的状态价值函数，我们称之为**最优状态价值函数 $V^{*}(s)$ **，表示为：
$$
V^{*}(s) = \mathop{\max}_{\pi}{V^{\pi}(s)}, \forall s \in \mathcal{S}
$$
同理，可以定义**最优动作价值函数 $Q^{*}(s, a)$ **：
$$
Q^{*}(s, a) = \mathop{\max}_{\pi}{Q^{\pi}(s, a)}, \forall s \in \mathcal{S}, a \in \mathcal{A}
$$
依据定义，对于最优动作价值函数，我们有当前状态动作 $(s, a)$ 下，一定会执行最优策略，则：
$$
Q^{*}(s, a) = r(s, a) + \gamma \sum_{s^{'} \in \mathcal{S}} {P(s^{'}|s, a) V^{*}(s^{'})}
$$

### 3.7.1 贝尔曼最优方程

依据 $V^{*}(s)$ 和 $Q^{*}(s, a)$ 的定义，我们可以得到贝尔曼最优方程：
$$
V^{*}(s) = \mathop{\max}_{a \in \mathcal{A}} \{r(s, a) + \gamma \sum_{s^{'} \in \mathcal{S}} {p(s^{'}|s, a}) V^{*}(s^{'})\}
$$

$$
Q^{*}(s, a) = r(s, a) + \gamma \sum_{s^{'} \in \mathcal{S}} {p(s^{'}|s, a) \mathop{\max}_{a^{'} \in \mathcal{A}} {Q^{*}(s^{'}, a^{'})}}
$$



