# 马尔可夫决策过程

## 3.1 简介

马尔可夫决策过程(Markov decision process，MDP)是强化学习的重要概念。**强化学习中的环境一般就是一个马尔可夫决策过程**。

与多臂老虎机问题不同，马尔可夫决策过程包含**状态信息**以及状态之间的**转移机制**。如果要用强化学习去解决一个实际问题，第一步要做的事情就是把这个实际问题抽象为一个马尔可夫决策过程，也就是明确马尔可夫决策过程的各个组成要素。

## 3.2 马尔可夫过程

### 3.2.1 随机过程

随机过程(stochastic process)的研究对象是**随时间演变的随机现象**，例如天气随时间的变化、城市交通随时间的变化。在随机过程中，随机现象在某时刻的取值是一个向量随机变量，用 $S_t$ 表示，所有可能的状态组成状态集合 $S$ 。随机现象便是状态的变化过程。在某时刻 $t$ 的状态 $S_t$ 通常取决于时刻之前的状态。我们将已知历史信息 $(S_1,..., S_t)$ 下一个时刻状态 $S_{t+1}$ 为的概率表示成 $P(S_{t+1}|S_1, ..., S_t)$。

### 3.2.2 马尔可夫性质

一个随机过程被称为具有**马尔可夫性质**(Markov property)，当且仅当某时刻的状态**只取决于上一时刻的状态**，用公式表示为
$$
P(S_{t+1}|S_t) = P(S_{t+1}|S_1, ..., S_t)
$$
也就是说，当前状态是未来的**充分统计量**，即下一个状态只取决于当前状态，而不会受到过去状态的影响。需要明确的是，具有马尔可夫性并不代表这个随机过程就和历史完全没有关系。因为虽然 $t+1$ 时刻的状态只与 $t$ 时刻的状态有关，但是 $t$ 时刻的状态其实包含了 $t-1$ 时刻的状态的信息，**通过这种链式的关系，历史的信息被传递到了现在**。

### 3.2.3 马尔可夫过程

**马尔可夫过程**(Markov process)指具有马尔可夫性质的随机过程，也被称为**马尔可夫链**(Markov chain)。我们通常用**元组 $<\mathcal{S}, \mathcal{P}>$** 描述一个马尔可夫过程，其中 $\mathcal{S}$ 是有限数量的**状态集合**， $\mathcal{P}$ 是**状态转移矩阵**(state transition matrix)。

假设一共有 $n$ 个状态，此时 $S=\{s_1, ..., s_n\}$ 。状态转移矩阵 $\mathcal{P}$ 定义了所有状态对之间的转移概率，即
$$
P=\begin{bmatrix}
P\{s_1|s_1\} & \cdots & P\{s_n|s_1\} \\
\vdots & \ddots & \vdots \\
P\{s_1|s_n\} & \cdots & P\{s_n|s_n\}
\end{bmatrix}
$$
矩阵的 $P_{ij}$ 元素表示 $P(s_j|s_i)=P(S_{t+1}=s_j|S_t=s_i)$表示从状态 $s_i$ 转移到状态 $s_j$ 的概率。由于从某个状态出发，到达其他状态的概率和必须为1，因此
$$
\sum_{j=1}^{n}P_{ij}=1
$$
以下展示一个具有6个状态的马尔可夫过程，其中 $S_6$ 被称为终止状态，因为它不会转移到其他状态。

![img](https://hrl.boyuai.com/static/markov-process.487c21a3.png)

给定一个马尔可夫过程，我们就可以从某个状态出发，根据它的状态转移矩阵生成一个状态**序列**(episode)，这个步骤也被叫做**采样**(sampling)。

## 3.3 马尔可夫奖励过程

在马尔可夫过程的基础上加入奖励函数 $r$ 和折扣因子 $\gamma$ ，就可以得到**马尔可夫奖励过程**(Markov reward process)。一个马尔可夫奖励过程由 $<\mathcal{S}, \mathcal{P}, r, \gamma>$ 构成，各个组成元素的含义如下所示：

- $\mathcal{S}$ 是**有限状态的集合**
- $\mathcal{P}$ 是**状态转移矩阵**
-  $r$ 是**奖励函数**，某个状态 $s$ 的奖励 $r(s)$ 指转移到该状态时可以获得奖励的期望

- $\gamma$ 是**折扣因子**(discount factor)，$\gamma$ 的取值范围为 $[0,1)$ 。引入折扣因子的理由是**远期利益具有一定不确定性**，所以我们需要对远期利益打一些折扣——接近1的 $\gamma$ 更关注长期的累计奖励，接近0的 $\gamma$ 更考虑短期奖励

### 3.3.1 回报

在一个马尔可夫奖励过程中，从第 $t$ 时刻状态 $S_t$ 开始，直到终止状态时，所有奖励的衰减之和称为**回报 $G_t$ **(Return)，公式如下：
$$
G_t = R_t + \gamma R_{t+1} + \gamma^2 R_{t+2} + ... = \sum_{k=0}^{\infty}{\gamma^k R_{t+k}}
$$
其中，$R_t$ 表示在时刻 $t$ 获得的奖励。下图继续使用之前马尔可夫过程的例子，并在其基础上添加**奖励函数**，构建成一个马尔可夫奖励过程。

![img](https://hrl.boyuai.com/static/mrp.c1e62649.png)

下面用代码表示图中马尔可夫奖励过程：

```python
import numpy as np

# 定义随机数种子
np.random.seed(0)
# 定义状态转移概率矩阵P
P = [
    [0.9, 0.1, 0.0, 0.0, 0.0, 0.0],
    [0.5, 0.0, 0.5, 0.0, 0.0, 0.0],
    [0.0, 0.0, 0.0, 0.6, 0.0, 0.4],
    [0.0, 0.0, 0.0, 0.0, 0.3, 0.7],
    [0.0, 0.2, 0.3, 0.5, 0.0, 0.0],
    [0.0, 0.0, 0.0, 0.0, 0.0, 1.0],
]
P = np.array(P)

rewards = [-1, -2, -2, 10, 1, 0] # 定义各状态的奖励函数
gamma = 0.5 # 折扣因子

# 计算给定序列上，从某个索引开始到序列结束的回报
def compute_return(start_index, chain, gamma):
    '''
    计算回报，采用回溯计算的方式
    '''
    G = 0 # 总回报
    for i in reversed(range(start_index, len(chain))):
        G = G * gamma + rewards[chain[i]-1]

    return G

# 一个状态序列,s1-s2-s3-s6
chain = [1, 2, 3, 6]
start_index = 0
G = compute_return(start_index, chain, gamma)
print("根据本序列计算得到回报为：%s。" % G)
```

### 3.3.2 价值函数

在马尔可夫奖励过程中，一个状态的**期望回报**(即从这个状态出发的未来累积奖励的期望)被称为这个状态的**价值**(value)。所有状态的价值就组成了**价值函数**(value function)，价值函数的输入为某个状态，输出为这个状态的价值。我们将价值函数写成 $V(s) = \mathbb{E}[G_t|S_t=s]$ ，展开为
$$
V(s) = \mathbb{E}[G_t|S_t=s] = \mathbb{E}[R_t + \gamma R_{t+1} + ... | S_t=s] = \mathbb{E}[R_t + \gamma G_{t+1}| S_t=s]
$$
上式可以进一步简化：

- 即时奖励的期望正是奖励函数的输出，即 $\mathbb{E} [R_t|S_t=s] = r(s)$
- 等式中剩余部分 $\mathbb{E}[\gamma G_{t+1}| S_t=s]$ 可以根据从状态 $s$ 出发的转移概率得到，即 $\mathbb{E}[\gamma G_{t+1}| S_t=s]=\gamma \mathbb{E}[G_{t+1}| S_t=s]=\gamma \sum_{s^{'}\in S}{p(s^{'}|s)V(s^{'})}$ 

$$
V(s) = r(s) + \gamma \sum_{s^{'}\in S}{p(s^{'}|s)V(s^{'})}
$$

上式就是马尔可夫奖励过程中非常有名的**贝尔曼方程**(Bellman equation)。若一个马尔可夫奖励过程一共有 $n$ 个状态，即 $S=\{s_1, s_2, ..., s_n\}$ ，我们将所有状态的价值表示成一个列向量 $\mathcal{V} = [V(s_1), V(s_2), ..., V(s_n)]^T$，同理，将奖励函数写成一个列向量 $\mathcal{R} = [r(s_1), r(s_2), ..., r(s_n)]^T$ 。此时可以将贝尔曼方程写成矩阵的形式：
$$
\mathcal{V} = \mathcal{R} + \gamma \mathcal{P} \mathcal{V}
$$
可以直接对上式进行求解，得到：
$$
(I - \gamma \mathcal{P})\mathcal{V} = \mathcal{R} \\
\mathcal{V} = (I-\gamma P)^{-1}\mathcal{R}
$$
上述方法求解的计算复杂度高，只能用于小规模的马尔可夫奖励过程。对于大规模情况，可以使用动态规划、蒙特卡洛、时序差分算法。

以下编写求解马尔可夫价值函数的代码：

```python
import numpy as np

# 定义随机数种子
np.random.seed(0)
# 定义状态转移概率矩阵P
P = [
    [0.9, 0.1, 0.0, 0.0, 0.0, 0.0],
    [0.5, 0.0, 0.5, 0.0, 0.0, 0.0],
    [0.0, 0.0, 0.0, 0.6, 0.0, 0.4],
    [0.0, 0.0, 0.0, 0.0, 0.3, 0.7],
    [0.0, 0.2, 0.3, 0.5, 0.0, 0.0],
    [0.0, 0.0, 0.0, 0.0, 0.0, 1.0],
]
P = np.array(P)

rewards = [-1, -2, -2, 10, 1, 0] # 定义各状态的奖励函数
gamma = 0.5 # 折扣因子

# 求解马尔可夫价值函数
def compute(P, rewards, gamma, states_num):
    '''
    计算贝尔曼方程的解析解
    '''
    rewards = np.array(rewards).reshape((-1,1))
    value = np.dot(np.linalg.inv(np.eye(states_num,states_num) - gamma * P), rewards)

    return value

V = compute(P, rewards, gamma, 6)
print("MRP中每个状态价值分别为\n", V)
```

## 3.4 马尔可夫决策过程

马尔可夫过程和马尔可夫奖励过程都是自发改变的随机过程；而如果有一个外界的“刺激”来共同改变这个随机过程，就有了**马尔可夫决策过程**(Markov decision process，MDP)。我们将这个来自外界的刺激称为**智能体**(agent)的动作，在马尔可夫奖励过程(MRP)的基础上加入动作，就得到了马尔可夫决策过程(MDP)。

**马尔可夫决策过程**由元组 $<\mathcal{S}, \mathcal{A}, \mathcal{P}, r, \gamma>$ 构成，其中：

- $\mathcal{S}$ 是状态的集合
- $\mathcal{A}$ 是动作的集合
- $P$ 是状态转移函数，其中 $P(s^{'}|s, a)$ 表示从状态 $s$ 采用动作 $a$ 到达状态 $s^{'}$ 的概率 
- $r$ 是奖励函数，这里 $r=r(s,a)$ 说明奖励同时取决于状态 $s$ 和动作 $a$
- $\gamma$ 是奖励的折扣因子

从上述概念可以看出MDP与MRP非常相像，主要区别为MDP中的**状态转移函数**和**奖励函数**都比MRP多了动作 $a$ 作为自变量。同时此时用状态转移函数 $P$ 代替了原本的状态转移矩阵，这样处理有两个优点：

- 状态转移函数和状态 $s$ 、动作 $a$ 均有关系，实际上变成了一个**三维数组**
- 状态转移函数更具有**一般性**，例如如果状态无限多(连续状态的MDP环境)，则状态转移矩阵难以表示

在马尔可夫决策过程(MDP)中，通常存在一个智能体来执行动作。马尔可夫决策过程是一个与时间相关的、不断进行的过程，在智能体和环境之间存在一个**不断交互**的过程。**交互过程**大致如下：

- 智能体根据当前状态 $S_t$ 选择动作 $A_t$
- 对于状态 $S_t$ 和动作 $A_t$，MDP根据奖励函数和状态转移函数得到 $S_{t+1}$ 和 $R_t$ 并反馈给智能体

![img](https://hrl.boyuai.com/static/rl-process.723b4a67.png)

### 3.4.1 策略

智能体的**策略**(Policy)通常用字母 $\pi$ 表示。策略 $\pi(a|s)=P(A_t=a|S_t=s)$ 是一个函数，表示在输入状态 $s$ 情况下采取动作 $a$ 的概率。

- 当一个策略是**确定性策略**时，它在每个状态时只输出一个**确定性的动作**，即只有该动作的概率为1，其他动作的概率为0
- 当一个策略是**随机性策略**时，它在每个状态时输出的是关于动作的概率分布，然后根据该分布进行**采样得到一个动作**

在MDP中，由于**马尔可夫性质**的存在，**策略只需要与当前状态有关，不需要考虑历史状态**。在 MDP 中也同样可以定义类似的价值函数，但此时的价值函数与策略有关，这意味着对于两个**不同的策略**来说，它们在同一个状态下的**价值也很可能是不同的**，可以这样理解：因为不同的策略会采取不同的动作，从而之后会遇到不同的状态，以及获得不同的奖励，所以它们的累积奖励的期望也就不同，即状态价值不同。

### 3.4.2 状态价值函数

我们用 $V^{\pi}(s)$ 表示在MDP中基于策略的**状态价值函数**(state-value function)，定义为从状态 $s$ 出发遵循策略 $\pi$ 能获得的**期望回报**，即：
$$
V^{\pi}(s) = \mathbb{E}_{\pi}{[G_t|S_t=s]}
$$

### 3.4.3 动作价值函数

不同于MRP，在MDP中，由于动作的存在，我们额外定义一个**动作价值函数**(action-value function)，用 $Q^{\pi}(s,a)$ 表示在MDP遵循策略 $\pi$ 时，对当前状态 $s$ 执行动作 $a$ 时的期望回报：
$$
Q^{\pi}(s,a) = \mathbb{E}_{\pi}[G_t|S_t=s, A_t=a]
$$
状态价值函数 $V^{\pi}(s)$ 和动作价值函数 $Q^{\pi}(s,a)$ 之间的关系：在使用策略 $\pi$ 时，状态 $s$ 的价值(基于状态价值函数定义的价值)等于在该状态下基于策略 $\pi$ 采取所有动作的概率与相应的动作价值相乘再求和的结果，即：
$$
V^{\pi}(s) = \sum_{a \in A}{\pi(a|s)Q^{\pi}(s,a)}
$$
比较下此时状态价值函数 $V^{\pi}(s)$ 和动作价值函数 $Q^{\pi}(s)$ 满足的贝尔曼方程：

- 状态价值函数 $V^{\pi}(s)$ 的贝尔曼方程

$$
V^{\pi}(s) = \sum_{a \in \mathcal{A}}{\pi(a|s)r(s,a)} + \gamma \sum_{a \in \mathcal{A}}{\pi(s|a)}\sum_{s^{'} \in \mathcal{S}}{P(s^{'}|s,a)V^{\pi}(s^{'})}
$$

- 动作价值函数 $Q^{\pi}(s)$ 的贝尔曼方程

$$
Q^{\pi}(s,a) = r(s,a) + \gamma \sum_{s^{'} \in \mathcal{S}}{P(s^{'}|s,a)V^{\pi}(s^{'})}
$$

### 3.4.4 贝尔曼期望方程

贝尔曼期望方程区别于贝尔曼方程，可以从上面的式子推导得到：
$$
V^{\pi}(s) = \sum_{a \in \mathcal{A}}{\pi(a|s)r(s,a)} + \gamma \sum_{a \in \mathcal{A}}{\pi(s|a)}\sum_{s^{'} \in \mathcal{S}}{P(s^{'}|s,a)V^{\pi}(s^{'})}
$$

$$
Q^{\pi}(s,a) = r(s,a) + \gamma \sum_{s^{'} \in \mathcal{S}}{P(s^{'}|s,a) \sum_{a^{'} \in \mathcal{A}}{\pi(a^{'}|s^{'}) Q^{\pi}(s^{'}, a^{'})}}
$$

以下面图中的案例为例，展示马尔可夫决策过程：

![img](https://hrl.boyuai.com/static/mdp.aaacb46a.png)

图中共有五种状态，每个状态可以采取实线对应的动作，采取动作后会以虚线旁的概率(如果没有，则为1)转移至下一个状态，并得到对应奖励。

针对MDP，我们希望将其转化为更为熟悉的MRP，以计算一个策略 $\pi$ 的状态价值函数 $V^{\pi}(s)$ 。采用的方式是将策略的动作进行边缘化，就可以得到没有动作的MRP，具体来说，我们希望用 $r^{'}(s) \rightarrow r(s,a)$ ，用 $P^{'}(s^{'}|s)\rightarrow P(s^{'}|s,a)$ 以转化为不考虑动作的 $MRP:<\mathcal{S}, \mathcal{P^{'}}, r^{'}, \gamma>$，其中
$$
r^{'}(s) = \sum_{a \in \mathcal{A}} \pi(a|s) r(s,a)
$$

$$
P^{'}(s^{'}|s) = \sum_{a \in \mathcal{A}} \pi(a|s) P(s^{'}|s,a)
$$

首先定义状态、动作等基本信息：

```python
import numpy as np

# 马尔可夫决策过程示例
# 定义状态和动作集合
S = ['s1', 's2', 's3', 's4', 's5']
A = ['保持s1', '前往s1', '前往s2', '前往s3', '前往s4', '前往s5', '概率前往']

# 状态转移函数
P = {
    "s1-保持s1-s1": 1.0,
    "s1-前往s2-s2": 1.0,
    "s2-前往s1-s1": 1.0,
    "s2-前往s3-s3": 1.0,
    "s3-前往s4-s4": 1.0,
    "s3-前往s5-s5": 1.0,
    "s4-前往s5-s5": 1.0,
    "s4-概率前往-s2": 0.2,
    "s4-概率前往-s3": 0.4,
    "s4-概率前往-s4": 0.4,
}

# 奖励函数
R = {
    "s1-保持s1": -1,
    "s1-前往s2": 0,
    "s2-前往s1": -1,
    "s2-前往s3": -2,
    "s3-前往s4": -2,
    "s3-前往s5": 0,
    "s4-前往s5": 10,
    "s4-概率前往": 1,
}

gamma = 0.5  # 折扣因子
MDP = (S, A, P, R, gamma) # 马尔可夫决策过程的集合
```

我们考虑采用两种策略，分别计算状态价值函数：

- 随机策略`pi_1`

```python
# 策略1,随机策略
Pi_1 = {
    "s1-保持s1": 0.5,
    "s1-前往s2": 0.5,
    "s2-前往s1": 0.5,
    "s2-前往s3": 0.5,
    "s3-前往s4": 0.5,
    "s3-前往s5": 0.5,
    "s4-前往s5": 0.5,
    "s4-概率前往": 0.5,
}
```

- 非随机策略`pi_2`

```python
# 策略2
Pi_2 = {
    "s1-保持s1": 0.6,
    "s1-前往s2": 0.4,
    "s2-前往s1": 0.3,
    "s2-前往s3": 0.7,
    "s3-前往s4": 0.5,
    "s3-前往s5": 0.5,
    "s4-前往s5": 0.1,
    "s4-概率前往": 0.9,
}
```

解决问题的基本思路是转化为存在解析解的`MRP`问题进行求解，代码如下：

```python
# 结合策略和状态转移函数，将MDP下的状态转移函数，转化为MRP下状态转移函数
def convert_P_mdp2mrp(P_mdp, Pi, S, A):
    '''
    将给定策略下的MDP的状态转移函数转化为MRP的状态转移函数

    输入:
    P_mdp:MDP下的状态转移函数
    Pi:给定策略
    S:状态列表
    A:动作列表
    '''
    state_len = len(S)
    P_mrp = np.zeros((state_len, state_len)) # MRP下的状态转移函数初始化
    for state_start in S:
        state_start_idx = S.index(state_start) # 起始状态索引
        for action in A:
            cur_state_action_pair = state_start + '-' + action# 当前的状态动作对

            # 找到策略中的状态动作对
            if cur_state_action_pair in Pi.keys():
                possibility = Pi[cur_state_action_pair] # 指定动作的概率
                state_end_dict = {k.split(cur_state_action_pair)[1][1:]:v for k, v in P_mdp.items() if cur_state_action_pair in k}

                for state_end, reward in state_end_dict.items():
                    state_end_idx = S.index(state_end) # 终止状态索引
                    P_mrp[state_start_idx, state_end_idx] += possibility * reward

    return P_mrp
```

