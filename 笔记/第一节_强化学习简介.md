# 强化学习简介

## 1.1 强化学习定义

通过从交互中学习来实现目标的计算方法，其中作出决策机器用**智能体**(agent)表示，智能体作出的决策会进一步改变环境，影响智能体下一步行动。

![img](https://hrl.boyuai.com/static/11.da5ee18f.png)

三个重要的方面：

- 感知：感知环境状态。例如：机器狗感知地面情况。
- 决策：智能体依据当前状态计算达到目标状态需要采取的动作的过程。例如：无人驾驶汽车依据当前状态计算方向盘和油门等行动。
- 奖励：环境依据智能体的状态和智能体的动作，产生的标量信号作为奖励反馈。例如：无人驾驶汽车能否安全平稳行驶。

> **注意**：强化学习中决策、策略和行动是三个概念：
>
> - 决策是指从可用的动作中选择一个行动的过程。在强化学习中，智能体(agent)通过观察环境(environment)和当前状态(state)，基于其策略(policy)和价值估计(value estimates)来做出决策。决策可能基于先前的经验、探索、规则、甚至随机性。
> - 策略(policy)是一个映射，将状态映射到动作，描述了智能体在特定环境下应该采取哪些动作。在强化学习中，策略可以是确定性的，即对于每个状态只有一个确定的动作，也可以是随机的，即对于每个状态可以有多个可能的动作，每个动作有一个概率分布。
> - 行动(action)是智能体对其决策的具体实施，通常包括更新环境、修改状态和获取奖励等过程。

## 1.2 强化学习交互过程

在每一个时间步 $t$ ，智能体：

- 感知状态 $S_t$

- 获得奖励 $R_t$
- 执行动作 $A_t$

在每一个时间步 $t$ ，环境：

- 获得行动 $A_t$

- 给出状态 $S_{t+1}$
- 给出奖励 $R_{t+1}$

## 1.3 强化学习的环境

强化学习的智能体是在和一个动态环境的交互中完成序贯决策的。当智能体执行动作时，环境下一刻状态的概率分布将由当前状态和智能体的动作来共同决定，即

$$下一状态 \sim P(\cdot | 当前状态, 智能体动作)$$

## 1.4 强化学习的目标

在上述动态环境下，智能体和环境每次进行交互时，环境会产生相应的奖励信号，其往往由**实数标量**(scalar)来表示。整个交互过程的每一轮获得的奖励信号可以进行累加，形成智能体的整体回报(return)。**在强化学习中，我们关注回报的期望，并将其定义为价值(value)，这就是强化学习中智能体学习的优化目标。**

需要注意的是，根据**环境的动态性**我们可以知道，即使**环境和智能体策略**不变，智能体的**初始状态**也不变，智能体和环境交互产生的结果也很可能是**不同**的，对应获得的回报也会不同。

## 1.5 强化学习的数据

在强化学习中，数据是在智能体与环境交互的过程中得到的。因此，智能体的**策略不同**，与环境交互所产生的**数据分布就不同**。

强化学习中有一个关于数据分布的概念，叫作占用度量(occupancy measure)，其中归一化占用度量用于衡量在一个智能体决策与一个动态环境的交互过程中，采样到一个具体的状态动作对(state-action pair)的**概率分布**。

一个策略的**价值**对应着一个占用度量下对应**奖励的期望**，因此寻找**最优策略**对应着寻找**最优占用度量**。

## 1.6 强化学习的独特性

强化学习通过改变策略来调整智能体和环境交互数据的分布，进而优化目标，即**修改数据分布**而**目标函数不变**。
