# `DQN`算法

## 7.1 简介

在前面的算法中，我们采用的环境(悬崖漫步环境、冰湖环境)均具有离散的状态动作空间，可以用矩阵存储动作价值函数，但是当状态或者动作数量非常大的时候，这种做法就不适用了。例如，当**状态是一张RGB图像**时，假设图像大小为 $210 \times 160 \times 3$，此时一共有 $256^{210 \times 160 \times 3}$ 种状态，在计算机中存储这个数量级的值表格是不现实的。此外，**当状态或者动作连续**的时候，就有无限个状态动作对，我们更加无法使用这种表格形式来记录各个状态动作对的值。

对于这种情况，我们需要用**函数拟合**的方法来估计值，即将这个复杂的值表格视作数据，使用一个参数化的函数来拟合这些数据。很显然，这种函数拟合的方法存在一定的精度损失，因此被称为**近似方法**。`DQN`算法就是一种近似方法，可以用来解决**连续状态下离散动作**的问题。

## 7.2 `CartPole`环境

下图所示的车杆环境中，它的状态值就是连续的，动作值是离散的。

![img](https://hrl.boyuai.com/static/cartpole.e4a03ca5.gif)

在车杆环境中，杆的一端固定小车，小车作为智能体的任务是**通过左右移动保持车上的杆竖直**，游戏的终止条件如下：
- 倾斜度数过大
- 车子离初始位置左右的偏离程度过大
- 坚持时间到达 200 帧

智能体的状态是一个维数为4的向量，每一维都是连续的，动作是离散的，动作空间大小为 2，详见下面两张表。

<center>表1 智能体的状态空间</center>

| 维度 |     意义     |  最小值   |  最大值  |
| :--: | :----------: | :-------: | :------: |
|  0   |   车的位置   |  $-2.4$   |  $2.4$   |
|  1   |   车的速度   | $-\infty$ | $\infty$ |
|  2   |   杆的角度   | $-41.8°$  | $41.8°$  |
|  3   | 杆尖端的速度 | $-\infty$ | $\infty$ |

<center>表2 智能体的动作空间</center>

| 标号 |     动作     |
| :--: | :----------: |
|  0   | 向左移动小车 |
|  1   | 向右移动小车 |

在游戏中每坚持一帧，智能体能获得分数为 1 的奖励，坚持时间越长，则最后的分数越高，坚持 200 帧即可获得最高的分数。

## 7.3 `DQN`算法

由于状态空间是连续的，车杆环境中的动作价值函数可以使用**函数拟合**(function approximation)的思想，即采用一个神经网络来表示函数。
- 若动作空间是连续的，神经网络的输入是状态和动作，然后输出一个标量，表示在状态下采取动作能获得的价值。
- 若动作空间是离散的，除了可以采取动作连续情况下的做法，我们还可以只将状态输入到神经网络中，使其同时输出每一个动作的值。

通常`DQN`(包括Q-learning)只能处理动作离散的情况，因为在函数的更新过程中有 $\mathop{max}_{a}$ 这一操作。假设神经网络用来拟合函数 $Q$ 的参数是 $\omega$，即每一个状态下所有可能动作的值我们都能表示为 $Q_{\omega}(s, a)$。我们称用于拟合函数 $Q$ 的神经网络为**Q网络**。

![img](https://hrl.boyuai.com/static/640.46b13e89.png)

损失函数的建立可以先回顾下 $Q-learning$ 的更新规则——采用时序差分来更新动作价值函数 $Q(s, a)$：
$$
Q(s, a) \leftarrow Q(s, a) + \alpha [r + \gamma \mathop{max}_{a^{\prime} \in \mathcal{A}}{Q(s^{\prime}, a^{\prime})} - Q(s, a)]
$$
于是，对于一组数据 $\{(s_i, a_i, r_i, s^{\prime}_i)\}$，我们可以将Q网络的损失函数构造为均方误差的形式：
$$
\omega^{*} = \mathop{argmin}_{\omega}{\frac{1}{2N} \sum_{i=1}^N[Q_{\omega}(s_i, a_i) - (r_i + \gamma \mathop{max}_{a^{\prime}}{Q_{\omega}(s^{\prime}_i, a^{\prime})})}]^2
$$
我们就可以将 $Q-learning$ 扩展到神经网络形式——**深度 Q 网络**(deep Q network, DQN)算法。由于`DQN`是**离线策略算法**(offline)，因此我们在收集数据的时候可以使用一个 $\epsilon-greedy$ 策略来平衡探索与利用，将收集到的数据存储起来，在后续的训练中使用。`DQN`中还有两个非常重要的模块——**经验回放**和**目标网络**，它们能够帮助`DQN`取得稳定、出色的性能。

### 7.3.1 经验回放

在一般的有监督学习中，假设训练数据是独立同分布($i.i.d$)的，我们每次训练神经网络的时候从训练数据中随机采样一个或若干个数据来进行梯度下降，随着学习的不断进行，每一个训练数据会被使用多次。在原来的 $Q-learning$ 算法中，每一个数据只会用来更新一次值。为了更好地将 $Q-learning$ 和深度神经网络结合，`DQN`算法采用了**经验回放**(experience replay)方法，具体做法为维护一个**回放缓冲区(buffer)**，将每次从环境中**采样**得到的四元组数据 $(s, a, r, s^{\prime})$ 存储到回放缓冲区中，训练Q网络的时候再从回放缓冲区中随机采样若干数据来进行训练。这么做可以起到以下两个作用：

- **使样本满足独立假设**：在MDP中交互采样得到的数据本身不满足独立假设，因为存在时序相关。非独立同分布的数据对训练神经网络有很大的影响，会使神经网络拟合到最近训练的数据上。采用经验回放可以打破样本之间的相关性，让其满足独立假设。
- **提高样本效率**。每一个样本可以被使用多次，十分适合深度神经网络的梯度学习。