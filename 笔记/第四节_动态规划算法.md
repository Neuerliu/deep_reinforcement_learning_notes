# 动态规划算法

## 4.1 简介

**动态规划**(dynamic programming)是程序设计算法中非常重要的内容，能够高效解决一些经典问题，例如背包问题和最短路径规划。动态规划的基本思想是将待求解问题分解成若干个**子问题**，先求解子问题，然后从这些子问题的解得到目标问题的解。动态规划会**保存已解决的子问题的答案**，在求解目标问题的过程中，需要这些子问题答案时就可以直接利用，避免重复计算。

基于动态规划的强化学习算法主要有两种

- **策略迭代**(policy iteration)：包括策略评估(policy evaluation)和策略提升(policy improvement)，策略迭代中的策略评估使用贝尔曼期望方程来得到一个策略的状态价值函数。
- **价值迭代**(value iteration)：价值迭代直接使用贝尔曼最优方程来进行动态规划，得到最终的最优状态价值。

不同于蒙特卡洛方法和时序差分算法，基于动态规划的这两种强化学习算法要求事先知道环境的状态转移函数和奖励函数，也就是需要知道整个马尔可夫决策过程。在这样一个白盒环境中，不需要通过智能体和环境的大量交互来学习，可以**直接用动态规划求解状态价值函数**。但是使用动态规划求解的缺点如下：

- 现实中的白盒环境很少
- 策略迭代和价值迭代通常只适用于有限马尔可夫决策过程，即状态空间和动作空间是离散且有限的

## 4.2 悬崖漫步环境

悬崖漫步是一个非常经典的强化学习环境，它要求一个智能体从起点出发，避开悬崖行走，最终到达目标位置，其中有一个4×12的网格世界。

![img](https://hrl.boyuai.com/static/540.f28e3c6f.png)

- **状态空间**：每一个网格表示一个状态。智能体的起点是左下角的状态，目标是右下角的状态。
- **动作空间**：智能体在每一个状态都可以采取 4 种动作，即上、下、左、右。
- **状态转移函数**：如果智能体采取动作后触碰到边界墙壁则状态不发生改变，否则就会相应到达下一个状态。环境中有一段悬崖，智能体掉入悬崖或到达目标状态都会结束动作并回到起点，也就是说掉入悬崖或者达到目标状态是终止状态。
- **奖励函数**：智能体每走一步的奖励是−1，掉入悬崖的奖励是−100。

接下来建立悬崖漫步(Cliff Walking)环境：

```python
class CliffWalkingEnv:
    '''
    悬崖漫步环境
    '''
    def __init__(self, ncol=12, nrow=4):
        self.ncol = ncol # 行数
        self.nrow = nrow # 列数
        # 状态转移函数
        self.P = self.createP() 
        # p[state][action] = [(p, next_state, reward, done)]
        # 对应(转移概率, 下一个状态, 奖励, 是否处于终止状态)
    
    # 状态转移函数
    def createP(self):
        # 初始化
        # 状态转移函数
        P = [[[] for j in range(4)] for i in range(self.nrow * self.ncol)] # P中48个状态，每一个状态可以采用4中动作
        # 动作初始化
        change = [
            [0, -1],
            [0, 1],
            [-1, 0],
            [1, 0]
        ] # 分别对应上、下、左、右

        # 遍历状态转移函数每个(s, a)，定义状态转移概率
        # 悬崖位于self.nrow - 1, 且在这一列中j=0代表起始位置，j=self.ncol-1代表终点位置
        for i in range(self.nrow): # 当前y
            for j in range(self.ncol): # 当前x
                # (i,j)代表此时位置
                for a in range(4):
                    # 此时采取动作a
                    # 1.如果位置在悬崖或者目标状态(终点)，则无法进行任何交互，任何动作奖励为0
                    if i == self.nrow - 1 and j > 0:
                        P[i * self.ncol + j][a] = [(1, i * self.ncol + j, 0, True)]
                        continue # 跳出循环

                    # 2.其他位置
                    # 限制x位置不能超出左右边界
                    next_x = min(self.ncol - 1, max(0, j + change[a][0]))
                    # 限制y位置不能超出边界
                    next_y = min(self.nrow - 1, max(0, i + change[a][1]))
                    next_state = next_y * self.ncol + next_x
                    # 非终点和悬崖位置奖励为-1
                    reward = -1
                    done = False

                    # 判断其他位置的下一个位置是否在悬崖或终点
                    if next_y == self.nrow - 1 and next_x > 0:
                        done = True
                        # 判断是否在终点
                        if next_x != self.ncol - 1:
                            # 在悬崖而非终点
                            reward = -100
                    P[i * self.ncol + j][a] = [(1, next_state, reward, done)]

        return P
```

## 4.3 策略迭代算法

**策略迭代**是策略评估和策略提升不断循环交替，直至最后得到最优策略的过程。

### 4.3.1 策略评估

策略评估这一过程用来**计算一个策略的状态价值函数**。回顾下贝尔曼期望方程：
$$
V^{\pi}(s) = \sum_{a \in \mathcal{A}}{\pi(a|s)}(r(s, a) + \gamma \sum_{s^{\prime} \in \mathcal{S}}{p(s^{\prime}|s, a) V^{\pi}(s^{\prime})})
$$
其中，$\pi(a|s)$ 是策略 $\pi$ 在状态 $s$ 下采取动作 $a$ 的概率。可以看到，当知道奖励函数 $r(s, a)$ 和状态转移函数 $p(s^{\prime}|s, a)$ 时，我们可以根据下一个状态的价值来计算当前状态的价值。考虑所有的状态，就变成了用**上一轮的状态价值函数**来计算**当前这轮的状态价值函数**，即
$$
V^{k+1}(s) = \sum_{a \in \mathcal{A}}{\pi(a|s) (r(s, a) + \gamma \sum_{s^{\prime} \in \mathcal{S}}{p(s^{\prime}|s, a) V^{k}(s^{\prime})})}
$$
可以选定任意的 $V^{0}$ 作为初始点，依据贝尔曼期望方程，可以得知 $V^{k} = v^{\pi}$ 是以上更新公式的一个不动点。但是，策略评估会**耗费巨大的计算代价**，我们可以在 $\mathop{\max}_{s \in \mathcal{S}}{[V^{k+1}(s) - V^{k}(s)]}$ 变化不大时停止策略评估。

### 4.3.2 策略提升

我们可以**根据策略评估后的结果改进该策略**。假设此时对于策略 $\pi$ ，我们已经知道其价值 $V^{\pi}$ ，也就是知道了在策略 $\pi$ 下从每一个状态出发最终得到的期望回报。

接下来，我们考虑如何通过更新后的状态价值函数进行策略改进。假设智能体在状态 $s$ 下采取动作 $a$ ，之后的动作依旧遵循策略 $\pi$ ，此时得到的期望回报其实就是动作价值 $Q^{\pi}(s, a)$ 。如果我们有 $Q^{\pi}(s, a) > V^{\pi}(s)$ ，则说明在状态 $s$ 下采取动作 $a$ 会比原来的策略 $\pi(a|s)$ 得到更高的期望回报。以上假设只是针对一个状态，现在假设存在一个确定性策略 $\pi^{\prime}$，在任意一个状态 $s$ 下，都满足：
$$
Q^{\pi}(s, \pi^{\prime}(s)) \geq V^{\pi}(s)
$$
于是在任何状态 $s$ 下，我们有：
$$
V^{\pi^{\prime}}(s) \geq V^{\pi}(s)
$$
上式就是**策略提升定理**(policy improvement theorem)，我们可以直接**贪心地在每一个状态选择动作价值最大的动作**，也就是：
$$
\pi^{\prime}(s) = arg\mathop{max}_{a}{Q^{\pi}(s, a)} = arg\mathop{max}_{a}\{r(s, a) + \gamma \sum_{s^{\prime}} {P(s^{\prime}|s, a) V^{\pi}(s^{\prime})} \}
$$

根据贪心法选取动作从而得到新的策略的过程称为**策略提升**。当策略提升之后得到的策略和之前的策略一样时，说明策略迭代达到了收敛，此时 $\pi$ 和 $\pi^{\prime}$ 就是最优策略。

策略提升定理的证明如下：
$$
V^{\pi}(s) \leq Q^{\pi}(s, \pi^{\prime}(s)) \\
= \mathbb{E}_{\pi^{\prime}}[R_t + \gamma V^{\pi}(S_{t+1}) | S_t=s] \\
\leq \mathbb{E}_{\pi^{\prime}}[R_t + \gamma Q^{\pi}(S_{t+1}, \pi^{\prime}(S_{t+1})) | S_t = s] \\
= \mathbb{E}_{\pi^{\prime}}[R_t + \gamma R_{t+1} + \gamma^2 V^{\pi}(S_{t+2}) | S_t = s] \\
\leq \mathbb{E}_{\pi^{\prime}}[R_t + \gamma R_{t+1} + \gamma^2 R_{t+2} + \gamma Q^{\pi}(S_{t+3}, \pi^{\prime}(S_{t+3})) | S_t = s] \\
\vdots \\
\leq \mathbb{E}_{\pi^{\prime}}[R_t + \gamma R_{t+1} + \gamma^2 R_{t+2} + \cdots | S_t = s] \\
= V^{\pi^{\prime}}(s)
$$

### 4.3.3 策略迭代算法

策略迭代算法的过程如下：对当前的策略进行策略评估，得到其状态价值函数，然后根据该状态价值函数进行策略提升以得到一个更好的新策略，接着继续评估新策略、提升策略……直至最后收敛到最优策略。上述流程写成伪代码的形式如下：

随机初始化策略 $\pi(s)$ 和价值函数 $V(s)$ 

// 策略评估

while $\Delta > \theta$ do:

​		$\Delta \leftarrow 0$

​		对于每一个状态 $s \in \mathcal{S}$ :

​				$v \leftarrow V(s)$

​				$V(s) \leftarrow r(s, \pi(s)) + \gamma \sum_{s^{\prime}}{P(s^{\prime} | s, \pi(s))V(s^{\prime})}$

​				$\Delta \leftarrow max(\Delta, |v-V(s)|)$

end while

// 策略改进

$\pi_{old} \leftarrow \pi$

对于每一个状态 $s \in \mathcal{S}$:

​		$\pi(s) \leftarrow \mathop{argmax}_{a}{r(s, a) + \gamma \sum_{s^{\prime}}{P(s^{\prime} | s, a)V(s^{\prime})}}$

若 $\pi_{old} = \pi$ ，则停止算法并返回 $V$ 和 $\pi$ ，否则转到策略评估环节。 

代码具体实现如下：

```python
# 基于悬崖漫步环境下的策略梯度法
# 策略迭代
class PolicyIteration:
    '''
    策略迭代算法
    '''
    def __init__(self, env, theta, gamma):
        self.env = env
        self.v = [0] * self.env.ncol * self.env.nrow # 状态价值函数初始化
        self.pi = [
            [0.25, 0.25, 0.25, 0.25] for i in range(self.env.ncol * self.env.nrow)
        ] # 策略的初始化
        self.theta = theta # 策略评估收敛阈值
        self.gamma = gamma # 折扣因子

    # 策略评估
    def policy_evaluation(self):
        cnt = 1 # 计数器
        while 1:
            max_diff = 0
            new_v = [0] * self.env.ncol * self.env.nrow # 新的状态价值函数初始化
            # 遍历状态，更新状态价值函数
            for s in range(self.env.ncol * self.env.nrow):
                qsa_list = [] # 当前状态s下所有Q(s,a)的值
                # 遍历动作
                for a in range(4):
                    qsa = 0
                    # 执行当前动作
                    # 遍历此时的状态转移函数
                    for res in self.env.P[s][a]:
                        # [(p, next_state, reward, done)] 
                        # 对应(转移概率, 下一个状态, 奖励, 是否处于终止状态)
                        p, next_state, r, done = res
                        # 悬崖漫步环境比较特殊，当前步的奖励和下一个状态有关
                        qsa += p * (r + self.gamma * self.v[next_state] * (1 - done))

                    # 状态s下的所有的动作价值在策略pi下的期望
                    qsa_list.append(self.pi[s][a] * qsa)
                # 更新价值函数
                new_v[s] = sum(qsa_list)
                max_diff = max(max_diff, abs(new_v[s] - self.v[s])) # 状态价值函数更新前后的差别
            
            self.v = new_v # 更新状态价值函数
            if max_diff < self.theta:
                break # 满足收敛条件，则退出策略评估
            cnt += 1

        print("策略评估进行%d轮后完成" % cnt)

    # 策略改进
    def policy_improvement(self):
        # 遍历状态
        for s in range(self.env.nrow * self.env.ncol):
            qsa_list = []
            # 遍历动作
            for a in range(4):
                qsa = 0
                # 遍历(s,a)下状态转移函数
                for res in self.env.P[s][a]:
                    p, next_state, r, done = res
                    qsa += p * (r + self.gamma * self.v[next_state] * (1 - done))
                qsa_list.append(qsa)

            # 选择动作价值最大的动作
            maxq = max(qsa_list)
            cntq = qsa_list.count(maxq)  # 计算有几个动作得到了最大的Q值
            # 让这些动作均分概率
            self.pi[s] = [1 / cntq if q == maxq else 0 for q in qsa_list]
        print("策略提升完成")
        return self.pi
    
    # 策略迭代
    def policy_iteration(self):
        while 1:
            self.policy_evaluation() # 策略评估
            old_pi = copy.deepcopy(self.pi) # 深拷贝策略
            new_pi = self.policy_improvement()
            if old_pi == new_pi:
                break # 停止策略迭代
```

为了可视化展现最终的策略，接下来增加一个打印策略的函数，用于打印当前策略在每个状态下的价值以及智能体会采取的动作。对于打印出来的动作，我们用`^o<o`表示等概率采取向左和向上两种动作，`ooo>`表示在当前状态只采取向右动作。

```python
import copy
from cliff_walking_env import CliffWalkingEnv

# 可视化策略                     
def print_agent(agent, action_meaning, disaster=[], end=[]):
    '''
    可视化策略

    输入:
    agent:智能体对象
    action_meaning:动作含义
    disaster:悬崖状态列表
    end:终点状态列表
    '''
    print("状态价值：")
    for i in range(agent.env.nrow):
        for j in range(agent.env.ncol):
            # 为了输出美观,保持输出6个字符
            print('%6.6s' % ('%.3f' % agent.v[i * agent.env.ncol + j]), end=' ')
        print("")

    print("策略：")
    for i in range(agent.env.nrow):
        for j in range(agent.env.ncol):
            # 一些特殊的状态,例如悬崖漫步中的悬崖
            if (i * agent.env.ncol + j) in disaster: # 悬崖
                print('****', end=' ')
            elif (i * agent.env.ncol + j) in end:  # 目标状态
                print('EEEE', end=' ')
            else:
                a = agent.pi[i * agent.env.ncol + j] # 采取动作a
                pi_str = ''
                for k in range(len(action_meaning)):
                    pi_str += action_meaning[k] if a[k] > 0 else 'o'
                print(pi_str, end=' ')
        print("")

if __name__ == '__main__':
    env = CliffWalkingEnv()
    action_meaning = ['^', 'v', '<', '>'] # 动作含义
    theta = 0.001
    gamma = 0.9
    # 采用策略迭代的智能体
    agent = PolicyIteration(env, theta, gamma)
    agent.policy_iteration()
    print_agent(agent, action_meaning, list(range(37, 47)), [47])
```

策略迭代算法的基本思路是：

- 首先通过策略评估，得到给定策略 $\pi$ 下的状态价值函数 $V^{\pi}$
- 接下来通过策略改进，得到更优策略 $\pi^{\prime}$
- 不断重复上述两个过程，直至策略 $\pi$ 和 $\pi^{\prime}$ 收敛

## 4.4 价值迭代算法

策略迭代中的**策略评估需要进行很多轮才能收敛**得到某一策略的状态函数，需要很大的计算量，尤其是在状态和动作空间比较大的情况下。如果**只在策略评估中进行一轮价值更新**，然后直接根据更新后的价值进行策略提升，这样是否可以呢？答案是肯定的，这就是**价值迭代算法**，它可以被认为是一种策略评估只进行了一轮更新的策略迭代算法。需要注意的是，价值迭代中不存在显式的策略，我们只维护一个状态价值函数。

价值迭代可以看成一种动态规划过程，它利用的是**贝尔曼最优方程**(不同于策略迭代使用的是贝尔曼期望方程)：
$$
V^{*}(s) = \mathop{\max}_{a \in \mathcal{A}} \{r(s, a) + \gamma \sum_{s^{'} \in \mathcal{S}} {p(s^{\prime}|s, a}) V^{*}(s^{\prime})\}
$$
将其写成迭代更新的方式为：
$$
V^{k+1}(s) = \mathop{max}_{a \in \mathcal{A}} \{r(s, a) + \gamma \sum_{s^{\prime}\in \mathcal{S}} {P(s^{\prime} | s, a)V^{k}(s^{\prime})}\}
$$
价值迭代便是按照以上更新方式进行的。等到 $V^{k+1}$ 和 $V^{k}$ 相同时，它就是贝尔曼最优方程的不动点，此时对应着最优状态价值函数 $V^{*}$ 。然后我们利用
$$
\pi(s) = \mathop{argmax}_{a} \{r(s, a) + \gamma \sum_{s^{\prime}} {p(s^{\prime} | s, a) V^{k+1}(s^{\prime})} \}
$$
从中恢复出最优策略即可，此时**不需要再比较策略是否收敛**。上述流程写成伪代码的形式如下：

随机初始化策略 $\pi(s)$ 和价值函数 $V(s)$ 

// 只进行一轮策略评估

while $\Delta > \theta$ do:

​		$\Delta \leftarrow 0$

​		对于每一个状态 $s \in \mathcal{S}$ :

​				$v \leftarrow V(s)$

​				$V(s) \leftarrow \mathop{max}_{a} \{r(s, a) + \gamma \sum_{s^{\prime}}{P(s^{\prime} | s, a)V(s^{\prime})}\}$ // 利用贝尔曼最优性方程

​				$\Delta \leftarrow max(\Delta, |v-V(s)|)$

end while

// 返回确定策略

$\pi(s) \leftarrow \mathop{argmax}_{a} \{r(s, a) + \gamma \sum_{s^{\prime}}{P(s^{\prime} | s, a)V(s^{\prime})}\}$

代码具体实现如下：

```python
import numpy as np
from cliff_walking_env import CliffWalkingEnv, print_agent

class ValueIteration:
    '''
    价值迭代算法

    输入:
    :param env: 环境
    :param theta: 策略评估收敛阈值
    :param gamma: 折扣因子
    '''
    def __init__(self, env, theta, gamma):
        self.env = env # 悬崖漫步环境
        self.v = [0] * self.env.ncol * self.env.nrow # 初始化状态价值函数
        self.theta = theta # 价值函数收敛的阈值
        self.gamma = gamma # 折扣因子
        # 价值迭代结束后得到的策略
        self.pi = [None for i in range(self.env.ncol * self.env.nrow)]

    # 价值迭代
    def value_iteration(self):
        cnt = 0
        while 1:
            max_diff = 0 # 记录价值迭代的次数
            new_v = [0] * self.env.ncol * self.env.nrow # 贝尔曼最优性方程更新最优价值函数
            # 遍历状态
            for s in range(self.env.ncol * self.env.nrow):
                qsa_list = [] # 计算状态s下Q(s, a)的值
                # 遍历动作
                for a in range(4):
                    qsa = 0
                    # 遍历状态转移函数
                    for res in self.env.P[s][a]:
                        p, next_state, r, done = res
                        # 当前Q(s,a)的值
                        qsa += p * (r + self.gamma * self.v[next_state] * (1-done))
                    qsa_list.append(qsa)

                new_v[s] = max(qsa_list) # 贝尔曼最优性方程，更新最优价值函数
                max_diff = max(max_diff, abs(new_v[s] - self.v[s]))
            self.v = new_v
            # 如果价值函数收敛
            if max_diff < self.theta:
                break
            cnt += 1
        print("价值迭代一共进行%d轮" % cnt)
        self.get_policy()

    # 获取策略
    def get_policy(self):
        # 计算给定状态下的动作价值函数
        for s in range(self.env.nrow * self.env.ncol):
            qsa_list = []
            for a in range(4):
                qsa = 0
                for res in self.env.P[s][a]:
                    p, next_state, r, done = res
                    qsa += p * (r + self.gamma * self.v[next_state] * (1-done))
                qsa_list.append(qsa)

            # 依据当前状态下最大动作状态价值选择动作，作为策略
            maxq = max(qsa_list) # 最大价值
            cntq = qsa_list.count(maxq) # 最大价值的动作个数
            # 具有最大动作价值的动作均分概率，作为策略
            self.pi[s] = [1/cntq if q == maxq else 0 for q in qsa_list]

# 采用悬崖漫步环境
env = CliffWalkingEnv()
action_meaning = ['^', 'v', '<', '>'] # 动作含义
theta = 0.001
gamma = 0.9
agent = ValueIteration(env, theta, gamma) # 价值迭代
agent.value_iteration()
print_agent(agent, action_meaning, list(range(37, 47)), [47])
```

## 4.5 冰湖环境

**冰湖环境**的状态空间和动作空间是有限的，我们在该环境中也尝试一下**策略迭代算法**和**价值迭代算法**。冰湖是`OpenAI` `Gym`库中的一个环境，也是一个网格世界，大小为4x4。

- **状态空间**：每一个网格表示一个状态。智能体的起点是左上角的状态，目标是右下角的状态，中间还有若干冰洞。
- **动作空间**：智能体在每一个状态都可以采取 4 种动作，即上、下、左、右。
- **状态转移函数**：如果智能体采取动作后触碰到边界墙壁则状态不发生改变，否则就会相应到达下一个状态，此外每次行走都有一定的概率**滑行到附近的其它状态**。环境中有一段悬崖，智能体掉入冰洞或到达目标状态都会结束动作并回到起点，也就是说掉入冰洞或者达到目标状态是终止状态。
- **奖励函数**：智能体每走一步的奖励是0，到达目标的奖励为1。

![img](https://hrl.boyuai.com/static/520.c82b1bec.png)

首先，我们使用`openai`的`gym`库内置环境：

```python
import gym
from policy_iteration import PolicyIteration
from value_iteration import ValueIteration
from cliff_walking_env import print_agent

# 使用gym环境
env = gym.make('FrozenLake-v1', render_mode='rgb_array') # 还可以使用'human'
env = env.unwrapped # 解封装以访问状态转移矩阵
env.reset() # 重置环境并返回初始状态

env.render() # 环境渲染，通常是弹窗显示或者打印出可视化环境

holes = set() # 冰洞的集合初始化
ends = set() # 终点的集合初始化
for s in env.P: # 状态空间
    for a in env.P[s]: # 状态s下的动作空间
        # 遍历(s,a)下的概率转移矩阵
        for s_ in env.P[s][a]:
            # s_的形式为(p, next_state, r, is_hole)
            # 对应(转移概率, 下一个状态, 当前步奖励, 是否为终止状态)
            if s_[2] == 1.0: # 获得奖励为1 -> 终点
                ends.add(s_[1])
            if s_[3] == True: # 为终止状态
                holes.add(s_[1])

holes = holes - ends # 冰洞需要排除终点
print('冰洞的索引为:', holes)
print('目标的索引为:', ends)

# 终点左边一格的状态下各种可能动作
for a in env.P[14]:
    print(env.P[14][a])
```

接下来，我们分别使用策略迭代算法和价值迭代算法：

```python
# 尝试下策略迭代算法
print('-----------------------------------------')
print('策略迭代:')
action_meaning = ['<', 'v', '>', '^'] # 动作含义
theta = 1e-5 # 策略迭代阈值
gamma = 0.9 # 折扣因子
agent = PolicyIteration(env, theta, gamma)
agent.policy_iteration() # 策略迭代
print_agent(agent, action_meaning, list(holes), list(ends))

# 价值迭代
print('-----------------------------------------')
print('价值迭代:')
agent = ValueIteration(env, theta, gamma)
agent.value_iteration()
print_agent(agent, action_meaning, list(holes), list(ends))
```

从运行结果可以看出来，两种策略的结果是一样的，并且结果并不像之前的悬崖漫步环境一样直观，这是因为在冰湖环境中，智能体会随机滑向其他状态。
