# 动态规划算法

## 4.1 简介

**动态规划**(dynamic programming)是程序设计算法中非常重要的内容，能够高效解决一些经典问题，例如背包问题和最短路径规划。动态规划的基本思想是将待求解问题分解成若干个**子问题**，先求解子问题，然后从这些子问题的解得到目标问题的解。动态规划会**保存已解决的子问题的答案**，在求解目标问题的过程中，需要这些子问题答案时就可以直接利用，避免重复计算。

基于动态规划的强化学习算法主要有两种

- **策略迭代**(policy iteration)：包括策略评估(policy evaluation)和策略提升(policy improvement)，策略迭代中的策略评估使用贝尔曼期望方程来得到一个策略的状态价值函数。
- **价值迭代**(value iteration)：价值迭代直接使用贝尔曼最优方程来进行动态规划，得到最终的最优状态价值。

不同于蒙特卡洛方法和时序差分算法，基于动态规划的这两种强化学习算法要求事先知道环境的状态转移函数和奖励函数，也就是需要知道整个马尔可夫决策过程。在这样一个白盒环境中，不需要通过智能体和环境的大量交互来学习，可以**直接用动态规划求解状态价值函数**。但是使用动态规划求解的缺点如下：

- 现实中的白盒环境很少
- 策略迭代和价值迭代通常只适用于有限马尔可夫决策过程，即状态空间和动作空间是离散且有限的

## 4.2 悬崖漫步环境

悬崖漫步是一个非常经典的强化学习环境，它要求一个智能体从起点出发，避开悬崖行走，最终到达目标位置，其中有一个4×12的网格世界。

- **状态空间**：每一个网格表示一个状态。智能体的起点是左下角的状态，目标是右下角的状态。
- **动作空间**：智能体在每一个状态都可以采取 4 种动作，即上、下、左、右。
- **状态转移函数**：如果智能体采取动作后触碰到边界墙壁则状态不发生改变，否则就会相应到达下一个状态。环境中有一段悬崖，智能体掉入悬崖或到达目标状态都会结束动作并回到起点，也就是说掉入悬崖或者达到目标状态是终止状态。
- **奖励函数**：智能体每走一步的奖励是−1，掉入悬崖的奖励是−100。

接下来建立悬崖漫步(Cliff Walking)环境：

```python
class CliffWalkingEnv:
    '''
    悬崖漫步环境
    '''
    def __init__(self, ncol=12, nrow=4):
        self.ncol = ncol # 行数
        self.nrow = nrow # 列数
        # 状态转移函数
        self.P = self.createP() 
        # p[state][action] = [(p, next_state, reward, done)]
        # 对应(转移概率, 下一个状态, 奖励, 是否处于终止状态)
    
    # 状态转移函数
    def createP(self):
        # 初始化
        # 状态转移函数
        P = [[[] for j in range(4)] for i in range(self.nrow * self.ncol)] # P中48个状态，每一个状态可以采用4中动作
        # 动作初始化
        change = [
            [0, -1],
            [0, 1],
            [-1, 0],
            [1, 0]
        ] # 分别对应上、下、左、右

        # 遍历状态转移函数每个(s, a)，定义状态转移概率
        # 悬崖位于self.nrow - 1, 且在这一列中j=0代表起始位置，j=self.ncol-1代表终点位置
        for i in range(self.nrow): # 当前y
            for j in range(self.ncol): # 当前x
                # (i,j)代表此时位置
                for a in range(4):
                    # 此时采取动作a
                    # 1.如果位置在悬崖或者目标状态(终点)，则无法进行任何交互，任何动作奖励为0
                    if i == self.nrow - 1 and j > 0:
                        P[i * self.ncol + j][a] = [(1, i * self.ncol + j, 0, True)]
                        continue # 跳出循环

                    # 2.其他位置
                    # 限制x位置不能超出左右边界
                    next_x = min(self.ncol - 1, max(0, j + change[a][0]))
                    # 限制y位置不能超出边界
                    next_y = min(self.nrow - 1, max(0, i + change[a][1]))
                    next_state = next_y * self.ncol + next_x
                    # 非终点和悬崖位置奖励为-1
                    reward = -1
                    done = False

                    # 判断其他位置的下一个位置是否在悬崖或终点
                    if next_y == self.nrow - 1 and next_x > 0:
                        done = True
                        # 判断是否在终点
                        if next_x != self.ncol - 1:
                            # 在悬崖而非终点
                            reward = -100
                    P[i * self.ncol + j][a] = [(1, next_state, reward, done)]

        return P
```

## 4.3 策略迭代算法

**策略迭代**是策略评估和策略提升不断循环交替，直至最后得到最优策略的过程。

### 4.3.1 策略评估

策略评估这一过程用来**计算一个策略的状态价值函数**。回顾下贝尔曼期望方程：
$$
V^{\pi}(s) = \sum_{a \in \mathcal{A}}{\pi(a|s)}(r(s, a) + \gamma \sum_{s^{\prime} \in \mathcal{S}}{p(s^{\prime}|s, a) V^{\pi}(s^{\prime})})
$$
其中，$\pi(a|s)$ 是策略 $\pi$ 在状态 $s$ 下采取动作 $a$ 的概率。可以看到，当知道奖励函数 $r(s, a)$ 和状态转移函数 $p(s^{\prime}|s, a)$ 时，我们可以根据下一个状态的价值来计算当前状态的价值。考虑所有的状态，就变成了用**上一轮的状态价值函数**来计算**当前这轮的状态价值函数**，即
$$
V^{k+1}(s) = \sum_{a \in \mathcal{A}}{\pi(a|s) (r(s, a) + \gamma \sum_{s^{\prime} \in \mathcal{S}}{p(s^{\prime}|s, a) V^{k}(s^{\prime})})}
$$
可以选定任意的 $V^{0}$ 作为初始点，依据贝尔曼期望方程，可以得知 $V^{k} = v^{\pi}$ 是以上更新公式的一个不动点。但是，策略评估会**耗费巨大的计算代价**，我们可以在 $\mathop{\max}_{s \in \mathcal{S}}{[V^{k+1}(s) - V^{k}(s)]}$ 变化不大时停止策略评估。

### 4.3.2 策略提升

我们可以**根据策略评估后的结果改进该策略**。假设此时对于策略 $\pi$ ，我们已经知道其价值 $V^{\pi}$ ，也就是知道了在策略 $\pi$ 下从每一个状态出发最终得到的期望回报。

接下来，我们考虑如何通过更新后的状态价值函数进行策略改进。假设智能体在状态 $s$ 下采取动作 $a$ ，之后的动作依旧遵循策略 $\pi$ ，此时得到的期望回报其实就是动作价值 $Q^{\pi}(s, a)$ 。如果我们有 $Q^{\pi}(s, a) > V^{\pi}(s)$ ，则说明在状态 $s$ 下采取动作 $a$ 会比原来的策略 $\pi(a|s)$ 得到更高的期望回报。以上假设只是针对一个状态，现在假设存在一个确定性策略 $\pi^{\prime}$，在任意一个状态 $s$ 下，都满足：
$$
Q^{\pi}(s, \pi^{\prime}(s)) \geq V^{\pi}(s)
$$
于是在任何状态 $s$ 下，我们有：
$$
V^{\pi^{\prime}}(s) \geq V^{\pi}(s)
$$
上式就是**策略提升定理**(policy improvement theorem)，我们可以直接**贪心地在每一个状态选择动作价值最大的动作**，也就是：
$$
\pi^{\prime}(s) = arg\mathop{max}_{a}{Q^{\pi}(s, a)} = arg\mathop{max}_{a}\{r(s, a) + \gamma \sum_{s^{\prime}} {P(s^{\prime}|s, a) V^{\pi}(s^{\prime})} \}
$$

根据贪心法选取动作从而得到新的策略的过程称为**策略提升**。当策略提升之后得到的策略和之前的策略一样时，说明策略迭代达到了收敛，此时 $\pi$ 和 $\pi^{\prime}$ 就是最优策略。

策略提升定理的证明如下：
$$
V^{\pi}(s) \leq Q^{\pi}(s, \pi^{\prime}(s)) \\
= \mathbb{E}_{\pi^{\prime}}[R_t + \gamma V^{\pi}(S_{t+1}) | S_t=s] \\
\leq \mathbb{E}_{\pi^{\prime}}[R_t + \gamma Q^{\pi}(S_{t+1}, \pi^{\prime}(S_{t+1})) | S_t = s] \\
= \mathbb{E}_{\pi^{\prime}}[R_t + \gamma R_{t+1} + \gamma^2 V^{\pi}(S_{t+2}) | S_t = s] \\
\leq \mathbb{E}_{\pi^{\prime}}[R_t + \gamma R_{t+1} + \gamma^2 R_{t+2} + \gamma Q^{\pi}(S_{t+3}, \pi^{\prime}(S_{t+3})) | S_t = s] \\
\vdots \\
\leq \mathbb{E}_{\pi^{\prime}}[R_t + \gamma R_{t+1} + \gamma^2 R_{t+2} + \cdots | S_t = s] \\
= V^{\pi^{\prime}}(s)
$$

### 4.3.3 策略迭代算法

策略迭代算法的过程如下：对当前的策略进行策略评估，得到其状态价值函数，然后根据该状态价值函数进行策略提升以得到一个更好的新策略，接着继续评估新策略、提升策略……直至最后收敛到最优策略。上述流程携程伪代码的形式如下：

随机初始化策略 $\pi(s)$ 和价值函数 $V(s)$ 

// 策略评估

while $\Delta > \theta$ do:

​		$\Delta \leftarrow 0$

​		对于每一个状态 $s \in \mathcal{S}$ :

​				$v \leftarrow V(s)$

​				$V(s) \leftarrow r(s, \pi(s)) + \gamma \sum_{s^{\prime}}{P(s^{\prime} | s, \pi(s))V(s^{\prime})}$

​				$\Delta \leftarrow max(\Delta, |v-V(s)|)$

end while

// 策略改进

$\pi_{old} \leftarrow \pi$

对于每一个状态 $s \in \mathcal{S}$:

​		$\pi(s) \leftarrow \mathop{argmax}_{a}{r(s, a) + \gamma \sum_{s^{\prime}}{P(s^{\prime} | s, a)V(s^{\prime})}}$

若 $\pi_{old} = \pi$ ，则停止算法并返回 $V$ 和 $\pi$ ，否则转到策略评估环节。 

代码具体实现如下：

```python
```

