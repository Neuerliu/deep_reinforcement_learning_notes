# 时序差分算法

## 5.1 简介

动态规划算法要求**马尔可夫决策过程是已知的**，即要求与智能体交互的环境是完全已知的。在此条件下，智能体可以直接用**动态规划算法就可以解出最优价值或策略**。这就好比对于有监督学习任务，如果直接显式给出了数据的分布公式，那么也可以通过在期望层面上直接最小化模型的泛化误差来更新模型参数，并不需要采样任何数据点。

对于**大部分强化学习现实场景**，其马尔可夫决策过程的**状态转移概率是无法写出来的**，也就无法直接进行动态规划。在这种情况下，智能体只能和环境进行交互，通过采样到的数据来学习，这类学习方法统称为**无模型的强化学习**(model-free reinforcement learning)。

不同于动态规划算法，**无模型的强化学习算法不需要事先知道环境的奖励函数和状态转移函数，**而是直接使用和环境交互的过程中**采样到的数据来学习**。本章将要讲解无模型的强化学习中的两大经典算法：`Sarsa`和`Q-learning`，它们都是基于**时序差分**(temporal difference, TD)的强化学习算法。同时，本章还会引入一组概念：**在线策略学习**和**离线策略学习**。

- **在线策略学习**：**使用在当前策略下采样得到的样本进行学习**，一旦策略被更新，当前的样本就被放弃了
- **离线策略学习**：**使用经验回放池将之前采样得到的样本收集起来再次利用**，就好像使用脸盆接水后洗手。

因此，离线策略学习往往能够更好地利用历史数据，并具有更小的样本复杂度(算法达到收敛结果需要在环境中采样的样本数量)，这使其被更广泛地应用。

## 5.2 时序差分算法

**时序差分**是一种用来估计一个策略的价值函数的方法，它**结合了蒙特卡洛和动态规划算法**的思想。时序差分方法和蒙特卡洛的相似之处在于**可以从样本数据中学习**，不需要事先知道环境；和动态规划的相似之处在于**根据贝尔曼方程的思想**，利用后续状态的价值估计来更新当前状态的价值估计。

- 蒙特卡洛方法对价值函数的增量更新方式：

$$
V(s_t) \leftarrow V(s_t) + \alpha [G_t - V(s_t)]
$$

这里采用 $\alpha$ 替换 $\frac{1}{N(s)}$ ，表示对价值估计更新的步长。蒙特卡洛方法必须要等整个序列结束之后才能计算得到这一次的回报 $G_t$ ，而时序差分方法只需要当前步结束即可进行计算。

- 具体来说，时序差分算法用当前获得的奖励 $r_t$ 加上下一个状态的价值估计 $V(s_{t+1})$ 来作为在当前状态会获得的回报，即：

$$
V(s_t) \leftarrow V(s_t) + \alpha [r_t + \gamma V(s_{t+1}) - V(s_t)]
$$

其中 $r_t + \gamma V(s_{t+1}) - V(s_t)$ 通常被称为**时序差分误差**(TD error)，时序差分算法将其与步长 $\alpha$ 的乘积作为状态价值的更新量。可以用来代替的原因是：
$$
V_{\pi}(s) = \mathbb{E}_{\pi}[G_t | S_t = s] \\
= \mathbb{E}_{\pi}[\sum_{k=0}^{\infty} {\gamma^k R_{t+k}} | S_t = s] \\
= \mathbb{E}_{\pi}[R_{t} + \sum_{k=1}^{\infty} {\gamma^k R_{t+k}} | S_t = s] \\
= \mathbb{E}_{\pi}[R_t + \gamma V_{\pi}(S_{t+1}) | S_t = s] \\
$$
蒙特卡洛方法将上式第一行作为更新的目标，而时序差分算法将上式最后一行作为更新的目标。

## 5.3 Sarsa算法